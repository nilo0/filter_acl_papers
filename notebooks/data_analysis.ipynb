{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from acl_papers.analysis import analyse_abstract\n",
    "from acl_papers.modules import check_relevant\n",
    "from acl_papers import ACL_PAPER_PATH\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) \n",
    "\n",
    "MODEL=\"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.listdir(ACL_PAPER_PATH)\n",
    "data_path = os.path.join(ACL_PAPER_PATH, path[0])\n",
    "\n",
    "df = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row['processed']:\n",
    "        continue\n",
    "    else:\n",
    "        verdict=analyse_abstract(title=row['title'], abstract=row['abstract'], model=MODEL)\n",
    "        df.at[i, 'relevancy_status'] = verdict\n",
    "        df.at[i, 'processed'] = True\n",
    "        df.to_csv(data_path)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>paper_type</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>relevancy_status</th>\n",
       "      <th>publication</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>3378</td>\n",
       "      <td>3378</td>\n",
       "      <td>3378</td>\n",
       "      <td>3378</td>\n",
       "      <td>main</td>\n",
       "      <td>Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics</td>\n",
       "      <td>How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice. This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0 paper_type  \\\n",
       "3378          3378          3378          3378        3378       main   \n",
       "\n",
       "                                                                                     title  \\\n",
       "3378  Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       abstract  \\\n",
       "3378  How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice. This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small.   \n",
       "\n",
       "      pub_year                              relevancy_status publication  \\\n",
       "3378      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "\n",
       "      processed  \n",
       "3378       True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, None]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_json_values(col_value):\n",
    "    data = str(col_value)\n",
    "    if data.startswith('```json') and data.endswith('```'):\n",
    "        data = data[7:-3]\n",
    "    data = data.replace('\\n', '').replace(\"'\", '\"')\n",
    "    jdata = json.loads(data)\n",
    "    status = jdata.get('status')\n",
    "    vclass = jdata.get('class')\n",
    "    return [status, vclass]\n",
    "\n",
    "\n",
    "extract_json_values(df.at[2, 'relevancy_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['relevancy'] = df['relevancy_status'].apply(lambda x: extract_json_values(x)[0])\n",
    "df['relevant_domain'] = df['relevancy_status'].apply(lambda x: extract_json_values(x)[1])\n",
    "\n",
    "df[['paper_type','title','abstract','pub_year','relevancy_status','publication','processed','relevancy','relevant_domain']].to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_type</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>relevancy_status</th>\n",
       "      <th>publication</th>\n",
       "      <th>processed</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>relevant_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long</td>\n",
       "      <td>Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement</td>\n",
       "      <td>An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models’ reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions</td>\n",
       "      <td>Natural language definitions possess a recursive, self-explanatory semantic structure that can support representation learning methods able to preserve explicit conceptual relations and constraints in the latent space. This paper presents a multi-relational model that explicitly leverages such a structure to derive word embeddings from definitions. By automatically extracting the relations linking defined and defining terms from dictionaries, we demonstrate how the problem of learning word embeddings can be formalised via a translational framework in Hyperbolic space and used as a proxy to capture the global semantic structure of definitions. An extensive empirical analysis demonstrates that the framework can help imposing the desired structural constraints while preserving the semantic mapping required for controllable and interpretable traversal. Moreover, the experiments reveal the superiority of the Hyperbolic word embeddings over the Euclidean counterparts and demonstrate that the multi-relational approach can obtain competitive results when compared to state-of-the-art neural models, with the advantage of being intrinsically more efficient and interpretable</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>long</td>\n",
       "      <td>Anisotropy Is Inherent to Self-Attention in Transformers</td>\n",
       "      <td>The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>long</td>\n",
       "      <td>Generating Benchmarks for Factuality Evaluation of Language Models</td>\n",
       "      <td>Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM’s propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create three benchmarks: Wiki-FACTOR, News-FACTOR and Expert-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>long</td>\n",
       "      <td>Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs</td>\n",
       "      <td>Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of indirect data leaking, where modelsare iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI’s GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI’s data usage policy, we extensively document the amount of data leaked to these models during the first year after the model’s release. We report that these models have been globally exposed to ∼4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>main</td>\n",
       "      <td>Diagnosing Vision-and-Language Navigation: What Really Matters</td>\n",
       "      <td>Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, there still exist non-negligible gaps between machines’ performance and human benchmarks. Moreover, the agents’ inner mechanisms for navigation decisions remain unclear. To the best of our knowledge, how the agents perceive the multimodal input is under-studied and needs investigation. In this work, we conduct a series of diagnostic experiments to unveil agents’ focus during navigation. Results show that indoor navigation agents refer to both object and direction tokens when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and poorly understand the object tokens. Transformer-based agents acquire a better cross-modal understanding of objects and display strong numerical reasoning ability than non-Transformer-based agents. When it comes to vision-and-language alignments, many models claim that they can align object tokens with specific visual targets. We find unbalanced attention on the vision and text input and doubt the reliability of such cross-modal alignments.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>main</td>\n",
       "      <td>Aligning to Social Norms and Values in Interactive Narratives</td>\n",
       "      <td>We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games—environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms—causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of people—in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others. We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>main</td>\n",
       "      <td>MOVER: Mask, Over-generate and Rank for Hyperbole Generation</td>\n",
       "      <td>Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale English hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole generation that does not require parallel literal-hyperbole pairs. During training, we fine-tune BART to infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of an input literal sentence and over-generate multiple possible hyperbolic versions. Then a BERT-based ranker selects the best candidate by hyperbolicity and paraphrase quality. Automatic and human evaluation results show that our model is effective at generating hyperbolic paraphrase sentences and outperforms several baseline systems.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>main</td>\n",
       "      <td>Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference</td>\n",
       "      <td>The task of natural language inference (NLI), to decide if a hypothesis entails or contradicts a premise, received considerable attention in recent years. All competitive systems build on top of contextualized representations and make use of transformer architectures for learning an NLI model. When somebody is faced with a particular NLI task, they need to select the best model that is available. This is a time-consuming and resource-intense endeavour. To solve this practical problem, we propose a simple method for predicting the performance without actually fine-tuning the model. We do this by testing how well the pre-trained models perform on the aNLI task when just comparing sentence embeddings with cosine similarity to what kind of performance is achieved when training a classifier on top of these embeddings. We show that the accuracy of the cosine similarity approach correlates strongly with the accuracy of the classification approach with a Pearson correlation coefficient of 0.65. Since the similarity is orders of magnitude faster to compute on a given dataset (less than a minute vs. hours), our method can lead to significant time savings in the process of model selection.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>main</td>\n",
       "      <td>Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics</td>\n",
       "      <td>How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice. This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3379 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_type  \\\n",
       "0          long   \n",
       "1          long   \n",
       "2          long   \n",
       "3          long   \n",
       "4          long   \n",
       "...         ...   \n",
       "3374       main   \n",
       "3375       main   \n",
       "3376       main   \n",
       "3377       main   \n",
       "3378       main   \n",
       "\n",
       "                                                                                              title  \\\n",
       "0     Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement   \n",
       "1                     Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions   \n",
       "2                                          Anisotropy Is Inherent to Self-Attention in Transformers   \n",
       "3                                Generating Benchmarks for Factuality Evaluation of Language Models   \n",
       "4         Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs   \n",
       "...                                                                                             ...   \n",
       "3374                                 Diagnosing Vision-and-Language Navigation: What Really Matters   \n",
       "3375                                  Aligning to Social Norms and Values in Interactive Narratives   \n",
       "3376                                   MOVER: Mask, Over-generate and Rank for Hyperbole Generation   \n",
       "3377          Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference   \n",
       "3378           Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             abstract  \\\n",
       "0                                                                                                                            An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models’ reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                       Natural language definitions possess a recursive, self-explanatory semantic structure that can support representation learning methods able to preserve explicit conceptual relations and constraints in the latent space. This paper presents a multi-relational model that explicitly leverages such a structure to derive word embeddings from definitions. By automatically extracting the relations linking defined and defining terms from dictionaries, we demonstrate how the problem of learning word embeddings can be formalised via a translational framework in Hyperbolic space and used as a proxy to capture the global semantic structure of definitions. An extensive empirical analysis demonstrates that the framework can help imposing the desired structural constraints while preserving the semantic mapping required for controllable and interpretable traversal. Moreover, the experiments reveal the superiority of the Hyperbolic word embeddings over the Euclidean counterparts and demonstrate that the multi-relational approach can obtain competitive results when compared to state-of-the-art neural models, with the advantage of being intrinsically more efficient and interpretable   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                  Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM’s propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create three benchmarks: Wiki-FACTOR, News-FACTOR and Expert-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.   \n",
       "4                                                                                                                                                                        Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of indirect data leaking, where modelsare iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI’s GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI’s data usage policy, we extensively document the amount of data leaked to these models during the first year after the model’s release. We report that these models have been globally exposed to ∼4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...   \n",
       "3374                                                                                                                                                                                                               Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, there still exist non-negligible gaps between machines’ performance and human benchmarks. Moreover, the agents’ inner mechanisms for navigation decisions remain unclear. To the best of our knowledge, how the agents perceive the multimodal input is under-studied and needs investigation. In this work, we conduct a series of diagnostic experiments to unveil agents’ focus during navigation. Results show that indoor navigation agents refer to both object and direction tokens when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and poorly understand the object tokens. Transformer-based agents acquire a better cross-modal understanding of objects and display strong numerical reasoning ability than non-Transformer-based agents. When it comes to vision-and-language alignments, many models claim that they can align object tokens with specific visual targets. We find unbalanced attention on the vision and text input and doubt the reliability of such cross-modal alignments.   \n",
       "3375  We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games—environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms—causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of people—in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others. We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.   \n",
       "3376                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale English hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole generation that does not require parallel literal-hyperbole pairs. During training, we fine-tune BART to infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of an input literal sentence and over-generate multiple possible hyperbolic versions. Then a BERT-based ranker selects the best candidate by hyperbolicity and paraphrase quality. Automatic and human evaluation results show that our model is effective at generating hyperbolic paraphrase sentences and outperforms several baseline systems.   \n",
       "3377                                                                                                                                                                                                                                                                                                                                    The task of natural language inference (NLI), to decide if a hypothesis entails or contradicts a premise, received considerable attention in recent years. All competitive systems build on top of contextualized representations and make use of transformer architectures for learning an NLI model. When somebody is faced with a particular NLI task, they need to select the best model that is available. This is a time-consuming and resource-intense endeavour. To solve this practical problem, we propose a simple method for predicting the performance without actually fine-tuning the model. We do this by testing how well the pre-trained models perform on the aNLI task when just comparing sentence embeddings with cosine similarity to what kind of performance is achieved when training a classifier on top of these embeddings. We show that the accuracy of the cosine similarity approach correlates strongly with the accuracy of the classification approach with a Pearson correlation coefficient of 0.65. Since the similarity is orders of magnitude faster to compute on a given dataset (less than a minute vs. hours), our method can lead to significant time savings in the process of model selection.   \n",
       "3378                                                                                                                                                                                                                                                                                                                                                                                                                                        How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice. This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small.   \n",
       "\n",
       "      pub_year                              relevancy_status publication  \\\n",
       "0         2024  ```json\\n{'status':false, 'class':null}\\n```        EACL   \n",
       "1         2024  ```json\\n{'status':false, 'class':null}\\n```        EACL   \n",
       "2         2024  ```json\\n{'status':false, 'class':null}\\n```        EACL   \n",
       "3         2024  ```json\\n{'status':false, 'class':null}\\n```        EACL   \n",
       "4         2024  ```json\\n{'status':false, 'class':null}\\n```        EACL   \n",
       "...        ...                                           ...         ...   \n",
       "3374      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3375      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3376      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3377      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3378      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "\n",
       "      processed  relevancy relevant_domain  \n",
       "0          True      False             NaN  \n",
       "1          True      False             NaN  \n",
       "2          True      False             NaN  \n",
       "3          True      False             NaN  \n",
       "4          True      False             NaN  \n",
       "...         ...        ...             ...  \n",
       "3374       True      False             NaN  \n",
       "3375       True      False             NaN  \n",
       "3376       True      False             NaN  \n",
       "3377       True      False             NaN  \n",
       "3378       True      False             NaN  \n",
       "\n",
       "[3379 rows x 9 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>paper_type</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>relevancy_status</th>\n",
       "      <th>publication</th>\n",
       "      <th>processed</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>relevant_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>short</td>\n",
       "      <td>A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation</td>\n",
       "      <td>Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models’ performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "      <td>long</td>\n",
       "      <td>Rethinking Tabular Data Understanding with Large Language Models</td>\n",
       "      <td>Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WikiTableQuestions, representing a substantial advancement over previous existing table processing paradigms of LLMs.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>long</td>\n",
       "      <td>E5: Zero-shot Hierarchical Table Analysis using Augmented LLMs via Explain, Extract, Execute, Exhibit and Extrapolate</td>\n",
       "      <td>Analyzing large hierarchical tables with multi-level headers presents challenges due to their complex structure, implicit semantics, and calculation relationships. While recent advancements in large language models (LLMs) have shown promise in flat table analysis, their application to hierarchical tables is constrained by the reliance on manually curated exemplars and the model’s token capacity limitations. Addressing these challenges, we introduce a novel code-augmented LLM-based framework, E5, for zero-shot hierarchical table question answering. This approach encompasses self-explaining the table’s hierarchical structures, code generation to extract relevant information and apply operations, external code execution to prevent hallucinations, and leveraging LLMs’ reasoning for final answer derivation. Empirical results indicate that our method, based on GPT-4, outperforms state-of-the-art fine-tuning methods with a 44.38 Exact Match improvement. Furthermore, we present F3, an adaptive algorithm designed for token-limited scenarios, effectively condensing large tables while maintaining useful information. Our experiments prove its efficiency, enabling the processing of large tables even with models having limited context lengths. The code is available at https://github.com/zzh-SJTU/E5-Hierarchical-Table-Analysis.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "      <td>long</td>\n",
       "      <td>FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering</td>\n",
       "      <td>Table Question Answering (TQA) aims at composing an answer to a question based on tabular data. While prior research has shown that TQA models lack robustness, understanding the underlying cause and nature of this issue remains predominantly unclear, posing a significant obstacle to the development of robust TQA systems. In this paper, we formalize three major desiderata for a fine-grained evaluation of robustness of TQA systems. They should (i) answer questions regardless of alterations in table structure, (ii) base their responses on the content of relevant cells rather than on biases, and (iii) demonstrate robust numerical reasoning capabilities. To investigate these aspects, we create and publish a novel TQA evaluation benchmark in English. Our extensive experimental analysis reveals that none of the examined state-of-the-art TQA systems consistently excels in these three aspects. Our benchmark is a crucial instrument for monitoring the behavior of TQA systems and paves the way for the development of robust TQA systems. We release our benchmark publicly.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>542</td>\n",
       "      <td>542</td>\n",
       "      <td>542</td>\n",
       "      <td>542</td>\n",
       "      <td>long</td>\n",
       "      <td>TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition</td>\n",
       "      <td>Table reasoning is a challenging task that requires understanding both natural language questions and structured tabular data. Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation, but they often struggle with large tables due to their limited input length. In this paper, we propose TabSQLify, a novel method that leverages text-to-SQL generation to decompose tables into smaller and relevant sub-tables, containing only essential information for answering questions or verifying statements, before performing the reasoning task. In our comprehensive evaluation on four challenging datasets, our approach demonstrates comparable or superior performance compared to prevailing methods reliant on full tables as input. Moreover, our method can reduce the input context length significantly, making it more scalable and efficient for large-scale table reasoning applications. Our method performs remarkably well on the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the TabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass other LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can reduce the table size significantly alleviating the computational load on LLMs when handling large tables without compromising performance.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>long</td>\n",
       "      <td>TableLlama: Towards Open Large Generalist Models for Tables</td>\n",
       "      <td>Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 5-44 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model’s generalizability. We open-source our dataset and trained model to boost future work on developing open generalist models for tables.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status': true, 'class': 'Table-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>630</td>\n",
       "      <td>630</td>\n",
       "      <td>630</td>\n",
       "      <td>630</td>\n",
       "      <td>long</td>\n",
       "      <td>Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in ToTTo</td>\n",
       "      <td>Neural Table-to-Text models tend to hallucinate, producing texts that contain factual errors. We investigate whether such errors in the output can be traced back to problems with the input. We manually annotated 1,837 texts generated by multiple models in the politics domain of the ToTTo dataset. We identify the input problems that are responsible for many output errors and show that fixing these inputs reduces factual errors by between 52% and 76% (depending on the model). In addition, we observe that models struggle in processing tabular inputs that are structured in a non-standard way, particularly when the input lacks distinct row and column values or when the column headers are not correctly mapped to corresponding values.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>697</td>\n",
       "      <td>697</td>\n",
       "      <td>697</td>\n",
       "      <td>697</td>\n",
       "      <td>long</td>\n",
       "      <td>Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning</td>\n",
       "      <td>An important open question in the use of large language models for knowledge-intensive tasks is how to effectively integrate knowledge from three sources: the model’s parametric memory, external structured knowledge, and external unstructured knowledge. Most existing prompting methods either rely on one or two of these sources, or require repeatedly invoking large language models to generate similar or identical content. In this work, we overcome these limitations by introducing a novel semi-structured prompting approach that seamlessly integrates the model’s parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs. Experimental results on open-domain multi-hop question answering datasets demonstrate that our prompting method significantly surpasses existing techniques, even exceeding those that require fine-tuning.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>short</td>\n",
       "      <td>Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?</td>\n",
       "      <td>Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging. Our study assesses LLMs’ proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance. We unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats. Our proposed FormatCoT aids in crafting format-specific instructions from the intended outputs to populate this benchmark. Addressing the gap in task-centered evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM performance. Our experiments show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial performance gains, outshining its LLM counterparts across most measures. In-depth error analysis and creating an ability map across six dimensions, coverage, formatting, reasoning, comprehension, pragmatics, and hallucination, highlight areas for future enhancements and suggest forthcoming research trajectories. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>short</td>\n",
       "      <td>Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata</td>\n",
       "      <td>The Directed Acyclic Transformer is a fast non-autoregressive (NAR) model that performs well in Neural Machine Translation. Two issues prevent its application to general Natural Language Generation (NLG) tasks: frequent Out-Of-Vocabulary (OOV) errors and the inability to faithfully generate entity names. We introduce Control-DAG, a constrained decoding algorithm for our Directed Acyclic T5 (DA-T5) model which offers lexical, vocabulary and length control. We show that Control-DAG significantly enhances DA-T5 on the Schema Guided Dialogue and the DART datasets, establishing strong NAR results for Task-Oriented Dialogue and Data-to-Text NLG.</td>\n",
       "      <td>2024</td>\n",
       "      <td>```json\\n{'status':true, 'class':'database-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>database-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>824</td>\n",
       "      <td>824</td>\n",
       "      <td>824</td>\n",
       "      <td>824</td>\n",
       "      <td>main</td>\n",
       "      <td>LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control</td>\n",
       "      <td>Logical Table-to-Text (LT2T) generation is tasked with generating logically faithful sentences from tables. There currently exists two challenges in the field: 1) Faithfulness: how to generate sentences that are factually correct given the table content; 2) Diversity: how to generate multiple sentences that offer different perspectives on the table. This work proposes LoFT, which utilizes logic forms as fact verifiers and content planners to control LT2T generation. Experimental results on the LogicNLG dataset demonstrate that LoFT is the first model that addresses unfaithfulness and lack of diversity issues simultaneously. Our code is publicly available at https://github.com/Yale-LILY/LoFT.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>835</td>\n",
       "      <td>835</td>\n",
       "      <td>835</td>\n",
       "      <td>835</td>\n",
       "      <td>main</td>\n",
       "      <td>Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters</td>\n",
       "      <td>Text generation from Abstract Meaning Representation (AMR) has substantially benefited from the popularized Pretrained Language Models (PLMs). Myriad approaches have linearized the input graph as a sequence of tokens to fit the PLM tokenization requirements. Nevertheless, this transformation jeopardizes the structural integrity of the graph and is therefore detrimental to its resulting representation. To overcome this issue, Ribeiro et al. (2021b) have recently proposed StructAdapt, a structure-aware adapter which injects the input graph connectivity within PLMs using Graph Neural Networks (GNNs). In this paper, we investigate the influence of Relative Position Embeddings (RPE) on AMR-to-Text, and, in parallel, we examine the robustness of StructAdapt. Through ablation studies, graph attack and link prediction, we reveal that RPE might be partially encoding input graphs. We suggest further research regarding the role of RPE will provide valuable insights for Graph-to-Text generation.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "      <td>main</td>\n",
       "      <td>Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models</td>\n",
       "      <td>Pretrained language models (PLMs) for data-to-text (D2T) generation can use human-readable data labels such as column headings, keys, or relation names to generalize to out-of-domain examples. However, the models are well-known in producing semantically inaccurate outputs if these labels are ambiguous or incomplete, which is often the case in D2T datasets. In this paper, we expose this issue on the task of descibing a relation between two entities. For our experiments, we collect a novel dataset for verbalizing a diverse set of 1,522 unique relations from three large-scale knowledge graphs (Wikidata, DBPedia, YAGO). We find that although PLMs for D2T generation expectedly fail on unclear cases, models trained with a large variety of relation labels are surprisingly robust in verbalizing novel, unseen relations. We argue that using data with a diverse set of clear and meaningful labels is key to training D2T generation systems capable of generalizing to novel domains.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>968</td>\n",
       "      <td>968</td>\n",
       "      <td>968</td>\n",
       "      <td>968</td>\n",
       "      <td>main</td>\n",
       "      <td>Semantic Parsing for Conversational Question Answering over Knowledge Graphs</td>\n",
       "      <td>In this paper, we are interested in developing semantic parsers which understand natural language questions embedded in a conversation with a user and ground them to formal queries over definitions in a general purpose knowledge graph (KG) with very large vocabularies (covering thousands of concept names and relations, and millions of entities). To this end, we develop a dataset where user questions are annotated with Sparql parses and system answers correspond to execution results thereof. We present two different semantic parsing approaches and highlight the challenges of the task: dealing with large vocabularies, modelling conversation context, predicting queries with multiple entities, and generalising to new questions at test time. We hope our dataset will serve as useful testbed for the development of conversational semantic parsers. Our dataset and models are released at https://github.com/EdinburghNLP/SPICE.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>990</td>\n",
       "      <td>990</td>\n",
       "      <td>990</td>\n",
       "      <td>990</td>\n",
       "      <td>main</td>\n",
       "      <td>The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents</td>\n",
       "      <td>We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation turns between agents working at Statistics Canada and online users looking for published data tables. The conversations stem from genuine intents, are held in English or French, and lead to agents retrieving one of over 5000 complex data tables. Based on this dataset, we propose two tasks: (1) automatic retrieval of relevant tables based on a on-going conversation, and (2) automatic generation of appropriate agent responses at each turn. We investigate the difficulty of each task by establishing strong baselines. Our experiments on a temporal data split reveal that all models struggle to generalize to future conversations, as we observe a significant drop in performance across both tasks when we move from the validation to the test set. In addition, we find that response generation models struggle to decide when to return a table. Considering that the tasks pose significant challenges to existing models, we encourage the community to develop models for our task, which can be directly used to help knowledge workers find relevant tables for live chat users.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>1030</td>\n",
       "      <td>1030</td>\n",
       "      <td>1030</td>\n",
       "      <td>1030</td>\n",
       "      <td>main</td>\n",
       "      <td>KGVL-BART: Knowledge Graph Augmented Visual Language BART for Radiology Report Generation</td>\n",
       "      <td>Timely generation of radiology reports and diagnoses is a challenge worldwide due to the enormous number of cases and shortage of radiology specialists. In this paper, we propose a Knowledge Graph Augmented Vision Language BART (KGVL-BART) model that takes as input two chest X-ray images- one frontal and the other lateral- along with tags which are diagnostic keywords, and outputs a report with the patient-specific findings. Our system development effort is divided into 3 stages: i) construction of the Chest X-ray KG (referred to as chestX-KG), ii) image feature extraction, and iii) training a KGVL-BART model using the visual, text, and KG data. The dataset we use is the well-known Indiana University Chest X-ray reports with the train, validation, and test split of 3025 instances, 300 instances, and 500 instances respectively. We construct a Chest X-Ray knowledge graph from these reports by extracting entity1-relation-entity2 triples; the triples get extracted by a rule-based tool of our own. Constructed KG is verified by two experienced radiologists (with experience of 30 years and 8 years, respectively). We demonstrate that our model- KGVL-BART- outperforms State-of-the-Art transformer-based models on standard NLG scoring metrics. We also include a qualitative evaluation of our system by experienced radiologist (with experience of 30 years) on the test data, which showed that 73% of the reports generated were fully correct, only 5.5% are completely wrong and 21.5% have important missing details though overall correct. To the best of our knowledge, ours is the first system to make use of multi-modality and domain knowledge to generate X-ray reports automatically.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>1101</td>\n",
       "      <td>1101</td>\n",
       "      <td>1101</td>\n",
       "      <td>1101</td>\n",
       "      <td>main</td>\n",
       "      <td>Knowledge Graph Compression Enhances Diverse Commonsense Generation</td>\n",
       "      <td>Generating commonsense explanations requires reasoning about commonsense knowledge beyond what is explicitly mentioned in the context. Existing models use commonsense knowledge graphs such as ConceptNet to extract a subgraph of relevant knowledge pertaining to concepts in the input. However, due to the large coverage and, consequently, vast scale of ConceptNet, the extracted subgraphs may contain loosely related, redundant and irrelevant information, which can introduce noise into the model. We propose to address this by applying a differentiable graph compression algorithm that focuses on the relevant knowledge for the task. The compressed subgraphs yield considerably more diverse outputs when incorporated into models for the tasks of generating commonsense and abductive explanations. Moreover, our model achieves better quality-diversity tradeoff than a large language model with 100 times the number of parameters. Our generic approach can be applied to additional NLP tasks that can benefit from incorporating external knowledge.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>1138</td>\n",
       "      <td>1138</td>\n",
       "      <td>1138</td>\n",
       "      <td>1138</td>\n",
       "      <td>main</td>\n",
       "      <td>QTSumm: Query-Focused Summarization over Tabular Data</td>\n",
       "      <td>People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users’ information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1196</td>\n",
       "      <td>1196</td>\n",
       "      <td>1196</td>\n",
       "      <td>1196</td>\n",
       "      <td>main</td>\n",
       "      <td>CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data</td>\n",
       "      <td>Large language models (LLMs) show powerful reasoning abilities on various text-based tasks. However, their reasoning capability on structured data such as tables has not been systematically explored. In this work, we first establish a comprehensive taxonomy of reasoning and operation types for tabular data analysis. Then, we construct a complex reasoning QA dataset over tabular data, named CRT-QA dataset (Complex Reasoning QA over Tabular data), with the following unique features: (1) it is the first Table QA dataset with multi-step operation and informal reasoning; (2) it contains fine-grained annotations on questions’ directness, composition types of sub-questions, and human reasoning paths which can be used to conduct a thorough investigation on LLMs’ reasoning ability; (3) it contains a collection of unanswerable and indeterminate questions that commonly arise in real-world situations. We further introduce an efficient and effective tool-augmented method, named ARC (Auto-exemplar-guided Reasoning with Code), to use external tools such as Pandas to solve table reasoning tasks without handcrafted demonstrations. The experiment results show that CRT-QA presents a strong challenge for baseline methods and ARC achieves the best result.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>1213</td>\n",
       "      <td>1213</td>\n",
       "      <td>1213</td>\n",
       "      <td>1213</td>\n",
       "      <td>main</td>\n",
       "      <td>TempTabQA: Temporal Question Answering for Semi-Structured Tables</td>\n",
       "      <td>Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on semi-structured tables. We present a dataset, TEMPTABQA, which comprises 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning more than 90 distinct domains. Using this dataset, we evaluate several state-of-the-art models for temporal reasoning. We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points. Given these results, our dataset has the potential to serve as a challenging benchmark to improve the temporal reasoning capabilities of NLP models.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>1236</td>\n",
       "      <td>1236</td>\n",
       "      <td>1236</td>\n",
       "      <td>1236</td>\n",
       "      <td>main</td>\n",
       "      <td>Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation</td>\n",
       "      <td>Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special “text critic” classifier, which guides the generation by assessing the match between the input data and the text generated so far. Our method does not need any changes to the underlying LM’s architecture or training procedure and can thus be combined with any model and decoding operating on word probabilities. The critic does not need any additional training data, using the base LM’s training data and synthetic negative examples. Our experimental results show that our method improves over the baseline on the WebNLG and OpenDialKG benchmarks.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>1292</td>\n",
       "      <td>1292</td>\n",
       "      <td>1292</td>\n",
       "      <td>1292</td>\n",
       "      <td>main</td>\n",
       "      <td>ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph</td>\n",
       "      <td>Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph (KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model (PLM) to model the question, and a graph neural network (GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin, even with fewer updated parameters and less training data. Our codes and data are publicly available at https://github.com/RUCAIBox/ReasoningLM.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>1327</td>\n",
       "      <td>1327</td>\n",
       "      <td>1327</td>\n",
       "      <td>1327</td>\n",
       "      <td>main</td>\n",
       "      <td>Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation</td>\n",
       "      <td>The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>1414</td>\n",
       "      <td>1414</td>\n",
       "      <td>1414</td>\n",
       "      <td>1414</td>\n",
       "      <td>main</td>\n",
       "      <td>Controllable Contrastive Generation for Multilingual Biomedical Entity Linking</td>\n",
       "      <td>Multilingual biomedical entity linking (MBEL) aims to map language-specific mentions in the biomedical text to standardized concepts in a multilingual knowledge base (KB) such as Unified Medical Language System (UMLS). In this paper, we propose Con2GEN, a prompt-based controllable contrastive generation framework for MBEL, which summarizes multidimensional information of the UMLS concept mentioned in biomedical text into a natural sentence following a predefined template. Instead of tackling the MBEL problem with a discriminative classifier, we formulate it as a sequence-to-sequence generation task, which better exploits the shared dependencies between source mentions and target entities. Moreover, Con2GEN matches against UMLS concepts in as many languages and types as possible, hence facilitating cross-information disambiguation. Extensive experiments show that our model achieves promising performance improvements compared with several state-of-the-art techniques on the XL-BEL and the Mantra GSC datasets spanning 12 typologically diverse languages.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>main</td>\n",
       "      <td>ReTAG: Reasoning Aware Table to Analytic Text Generation</td>\n",
       "      <td>The task of table summarization involves generating text that both succinctly and accurately represents the table or a specific set of highlighted cells within a table. While significant progress has been made in table to text generation techniques, models still mostly generate descriptive summaries, which reiterates the information contained within the table in sentences. Through analysis of popular table to text benchmarks (ToTTo (Parikh et al., 2020 and InfoTabs (Gupta et al., 2020) we observe that in order to generate the ideal summary, multiple types of reasoning is needed coupled with access to knowledge beyond the scope of the table. To address this gap, we propose ReTAG, a table and reasoning aware model that uses vector-quantization to infuse different types of analytical reasoning into the output. ReTAG achieves 2.2%, 2.9% improvement on the PARENT metric in the relevant slice of ToTTo and InfoTabs for the table to text generation task over state of the art baselines. Through human evaluation, we observe that output from ReTAG is upto 12% more faithful and analytical compared to a strong table-aware model. To the best of our knowledge, ReTAG is the first model that can controllably use multiple reasoning methods within a structure-aware sequence to sequence model to surpass state of the art performance in multiple table to text tasks. We extend (and open source 35.6K analytical, 55.9k descriptive instances) the ToTTo, InfoTabs datasets with the reasoning categories used in each reference sentences.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status': true, 'class': 'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>main</td>\n",
       "      <td>HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies</td>\n",
       "      <td>A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external models such as gpt-35-turbo. We propose a cooperative game dubbed “HiddenTables” as a potential resolution to this challenge. In essence, “HiddenTables” is played between the code-generating LLM “Solver” and the “Oracle” which evaluates the ability of the LLM agents to solve TableQA tasks. This game is based on natural language schemas and importantly, ensures the security of the underlying data. We provide evidential experiments on a diverse set of tables that demonstrate an LLM’s collective inability to generalize and perform on complex queries, handle compositional dependencies, and align natural language to programmatic commands when concrete table schemas are provided. Unlike encoder-based models, we have pushed the boundaries of “HiddenTables” to not be limited by the number of rows - therefore we exhibit improved efficiency in prompt and completion tokens. Our infrastructure has spawned a new dataset “PyQTax” that spans across 116,671 question-table-answer triplets and provides additional fine-grained breakdowns and labels for varying question taxonomies. Therefore, in tandem with our academic contributions regarding LLMs’ deficiency in TableQA tasks, “HiddenTables” is a tactile manifestation of how LLMs can interact with massive datasets while ensuring data security and minimizing generation costs.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status': true, 'class': 'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>1547</td>\n",
       "      <td>1547</td>\n",
       "      <td>1547</td>\n",
       "      <td>1547</td>\n",
       "      <td>main</td>\n",
       "      <td>SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables</td>\n",
       "      <td>Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>1562</td>\n",
       "      <td>1562</td>\n",
       "      <td>1562</td>\n",
       "      <td>1562</td>\n",
       "      <td>main</td>\n",
       "      <td>Enhancing Biomedical Lay Summarisation with External Knowledge Graphs</td>\n",
       "      <td>Previous approaches for automatic lay summarisation are exclusively reliant on the source article that, given it is written for a technical audience (e.g., researchers), is unlikely to explicitly define all technical concepts or state all of the background information that is relevant for a lay audience. We address this issue by augmenting eLife, an existing biomedical lay summarisation dataset, with article-specific knowledge graphs, each containing detailed information on relevant biomedical concepts. Using both automatic and human evaluations, we systematically investigate the effectiveness of three different approaches for incorporating knowledge graphs within lay summarisation models, with each method targeting a distinct area of the encoder-decoder model architecture. Our results confirm that integrating graph-based domain knowledge can significantly benefit lay summarisation by substantially increasing the readability of generated text and improving the explanation of technical concepts.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>1601</td>\n",
       "      <td>1601</td>\n",
       "      <td>1601</td>\n",
       "      <td>1601</td>\n",
       "      <td>main</td>\n",
       "      <td>Structure-aware Knowledge Graph-to-text Generation with Planning Selection and Similarity Distinction</td>\n",
       "      <td>The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. One of the primary challenges in this task is bridging the gap between the diverse structures of the KG and the target text, while preserving the details of the input KG. To address this, we propose a novel approach that efficiently integrates graph structure-aware modules with pre-trained language models. Unlike conventional techniques, which only consider direct connections between first-order neighbors, our method delves deeper by incorporating Relative Distance Encoding as a bias within the graph structure-aware module. This enables our model to better capture the intricate topology information present in the KG. To further elevate the fidelity of the generated text, Planning Selection and Similarity Distinction are introduced. Our approach filters the most relevant linearized sequences by employing a planning scorer, while simultaneously distinguishing similar input KGs through contrastive learning techniques. Experiments on two datasets demonstrate the superiority of our model.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status': true, 'class': 'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>1834</td>\n",
       "      <td>1834</td>\n",
       "      <td>1834</td>\n",
       "      <td>1834</td>\n",
       "      <td>main</td>\n",
       "      <td>Hallucination Mitigation in Natural Language Generation from Large-Scale Open-Domain Knowledge Graphs</td>\n",
       "      <td>In generating natural language descriptions for knowledge graph triples, prior works used either small-scale, human-annotated datasets or datasets with limited variety of graph shapes, e.g., those having mostly star graphs. Graph-to-text models trained and evaluated on such datasets are largely not assessed for more realistic large-scale, open-domain settings. We introduce a new dataset, GraphNarrative, to fill this gap. Fine-tuning transformer-based pre-trained language models has achieved state-of-the-art performance among graph-to-text models. However, this method suffers from information hallucination—the generated text may contain fabricated facts not present in input graphs. We propose a novel approach that, given a graph-sentence pair in GraphNarrative, trims the sentence to eliminate portions that are not present in the corresponding graph, by utilizing the sentence’s dependency parse tree. Our experiment results verify this approach using models trained on GraphNarrative and existing datasets. The dataset, source code, and trained models are released at https://github.com/idirlab/graphnarrator.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status': true, 'class': 'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>main</td>\n",
       "      <td>API-Assisted Code Generation for Question Answering on Varied Table Structures</td>\n",
       "      <td>A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures — relational, multi-table, and hierarchical matrix shapes — and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>1978</td>\n",
       "      <td>main</td>\n",
       "      <td>Large Language Models are Complex Table Parsers</td>\n",
       "      <td>With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibiting remarkable reasoning and comprehension abilities in Natural Language Processing (NLP), most Question Answering (QA) research has primarily centered around general QA tasks based on GPT, neglecting the specific challenges posed by Complex Table QA. In this paper, we propose to incorporate GPT-3.5 to address such challenges, in which complex tables are reconstructed into tuples and specific prompt designs are employed for dialogues. Specifically, we encode each cell’s hierarchical structure, position information, and content as a tuple. By enhancing the prompt template with an explanatory description of the meaning of each tuple and the logical reasoning process of the task, we effectively improve the hierarchical structure awareness capability of GPT-3.5 to better parse the complex tables. Extensive experiments and results on Complex Table QA datasets, i.e., the open-domain dataset HiTAB and the aviation domain dataset AIT-QA show that our approach significantly outperforms previous work on both datasets, leading to state-of-the-art (SOTA) performance.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>2046</td>\n",
       "      <td>2046</td>\n",
       "      <td>2046</td>\n",
       "      <td>2046</td>\n",
       "      <td>main</td>\n",
       "      <td>Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue</td>\n",
       "      <td>Knowledge-grounded dialogue is a task of gener- ating an informative response based on both the dialogue history and external knowledge source. In general, there are two forms of knowledge: manu- ally annotated knowledge graphs and knowledge text from website. From various evaluation viewpoints, each type of knowledge has advantages and downsides. To further distinguish the principles and determinants from the intricate factors, we conduct a thorough experiment and study on the task to answer three essential questions. The ques- tions involve the choice of appropriate knowledge form, the degree of mutual effects between knowl- edge and the model selection, and the few-shot performance of knowledge. Supported by statistical shreds of evidence, we offer conclusive solutions and sensible suggestions for directions and standards of future research.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>2067</td>\n",
       "      <td>2067</td>\n",
       "      <td>2067</td>\n",
       "      <td>2067</td>\n",
       "      <td>main</td>\n",
       "      <td>ToolWriter: Question Specific Tool Synthesis for Tabular Data</td>\n",
       "      <td>Tabular question answering (TQA) presents a challenging setting for neural systems by requiring joint reasoning of natural language with large amounts of semi-structured data. Unlike humans who use programmatic tools like filters to transform data before processing, language models in TQA process tables directly, resulting in information loss as table size increases. In this paper we propose ToolWriter to generate query specific programs and detect when to apply them to transform tables and align them with the TQA model’s capabilities. Focusing Toolwriter to generate row-filtering tools improves the state-of-the-art for WikiTableQuestions and WikiSQL with the most performance gained on long tables. By investigating headroom, our work highlights the broader potential for programmatic tools combined with neural components to manipulate large amounts of structured data.</td>\n",
       "      <td>2023</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>2150</td>\n",
       "      <td>2150</td>\n",
       "      <td>2150</td>\n",
       "      <td>2150</td>\n",
       "      <td>main</td>\n",
       "      <td>UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</td>\n",
       "      <td>Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>2233</td>\n",
       "      <td>2233</td>\n",
       "      <td>2233</td>\n",
       "      <td>2233</td>\n",
       "      <td>main</td>\n",
       "      <td>Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs</td>\n",
       "      <td>Answering factual questions with temporal intent over knowledge graphs (temporal KGQA) attracts rising attention in recent years.In the generation of temporal queries, existing KGQA methods ignore the fact that some intrinsic connections between events can make them temporally related, which may limit their capability.We systematically analyze the possible interpretation of temporal constraints and conclude the interpretation structures as the Semantic Framework of Temporal Constraints, SF-TCons.Based on the semantic framework, we propose a temporal question answering method, SF-TQA, which generates query graphs by exploring the relevant facts of mentioned entities, where the exploring process is restricted by SF-TCons. Our evaluations show that SF-TQA significantly outperforms existing methods on two benchmarks over different knowledge graphs.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>2282</td>\n",
       "      <td>2282</td>\n",
       "      <td>2282</td>\n",
       "      <td>2282</td>\n",
       "      <td>main</td>\n",
       "      <td>PRINCE: Prefix-Masked Decoding for Knowledge Enhanced Sequence-to-Sequence Pre-Training</td>\n",
       "      <td>Pre-trained Language Models (PLMs) have shown effectiveness in various Natural Language Processing (NLP) tasks. Denoising autoencoder is one of the most successful pre-training frameworks, learning to recompose the original text given a noise-corrupted one. The existing studies mainly focus on injecting noises into the input. This paper introduces a simple yet effective pre-training paradigm, equipped with a knowledge-enhanced decoder that predicts the next entity token with noises in the prefix, explicitly strengthening the representation learning of entities that span over multiple input tokens. Specifically, when predicting the next token within an entity, we feed masks into the prefix in place of some of the previous ground-truth tokens that constitute the entity. Our model achieves new state-of-the-art results on two knowledge-driven data-to-text generation tasks with up to 2% BLEU gains.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'database-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>database-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>2385</td>\n",
       "      <td>2385</td>\n",
       "      <td>2385</td>\n",
       "      <td>2385</td>\n",
       "      <td>main</td>\n",
       "      <td>CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation</td>\n",
       "      <td>Practical dialog systems need to deal with various knowledge sources, noisy user expressions, and the shortage of annotated data. To better solve the above problems, we propose CGoDial, a new challenging and comprehensive Chinese benchmark for multi-domain Goal-oriented Dialog evaluation. It contains 96,763 dialog sessions, and 574,949 dialog turns totally, covering three datasets with different knowledge sources: 1) a slot-based dialog (SBD) dataset with table-formed knowledge, 2) a flow-based dialog (FBD) dataset with tree-formed knowledge, and a retrieval-based dialog (RBD) dataset with candidate-formed knowledge. To bridge the gap between academic benchmarks and spoken dialog scenarios, we either collect data from real conversations or add spoken features to existing datasets via crowd-sourcing. The proposed experimental settings include the combinations of training with either the entire training set or a few-shot training set, and testing with either the standard test set or a hard test subset, which can assess model capabilities in terms of general prediction, fast adaptability and reliable robustness.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>2392</td>\n",
       "      <td>2392</td>\n",
       "      <td>2392</td>\n",
       "      <td>2392</td>\n",
       "      <td>main</td>\n",
       "      <td>DSM: Question Generation over Knowledge Base via Modeling Diverse Subgraphs with Meta-learner</td>\n",
       "      <td>Existing methods on knowledge base question generation (KBQG) learn a one-size-fits-all model by training together all subgraphs without distinguishing the diverse semantics of subgraphs. In this work, we show that making use of the past experience on semantically similar subgraphs can reduce the learning difficulty and promote the performance of KBQG models. To achieve this, we propose a novel approach to model diverse subgraphs with meta-learner (DSM). Specifically, we devise a graph contrastive learning-based retriever to identify semantically similar subgraphs, so that we can construct the semantics-aware learning tasks for the meta-learner to learn semantics-specific and semantics-agnostic knowledge on and across these tasks. Extensive experiments on two widely-adopted benchmarks for KBQG show that DSM derives new state-of-the-art performance and benefits the question answering tasks as a means of data augmentation.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>2432</td>\n",
       "      <td>2432</td>\n",
       "      <td>2432</td>\n",
       "      <td>2432</td>\n",
       "      <td>main</td>\n",
       "      <td>Self-supervised Graph Masking Pre-training for Graph-to-Text Generation</td>\n",
       "      <td>Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text (G2T) generation by processing the linearised version of a graph. However, the linearisation is known to ignore the structural information. Additionally, PLMs are typically pre-trained on free text which introduces domain mismatch between pre-training and downstream G2T generation tasks. To address these shortcomings, we propose graph masking pre-training strategies that neither require supervision signals nor adjust the architecture of the underlying pre-trained encoder-decoder model. When used with a pre-trained T5, our approach achieves new state-of-the-art results on WebNLG+2020 and EventNarrative G2T generation datasets. Our method also shows to be very effective in the low-resource setting.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>2471</td>\n",
       "      <td>2471</td>\n",
       "      <td>2471</td>\n",
       "      <td>2471</td>\n",
       "      <td>main</td>\n",
       "      <td>IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic Languages</td>\n",
       "      <td>Natural Language Generation (NLG) for non-English languages is hampered by the scarcity of datasets in these languages. We present the IndicNLG Benchmark, a collection of datasets for benchmarking NLG for 11 Indic languages. We focus on five diverse tasks, namely, biography generation using Wikipedia infoboxes, news headline generation, sentence summarization, paraphrase generation and, question generation. We describe the created datasets and use them to benchmark the performance of several monolingual and multilingual baselines that leverage pre-trained sequence-to-sequence models. Our results exhibit the strong performance of multilingual language-specific pre-trained models, and the utility of models trained on our dataset for other related NLG tasks. Our dataset creation methods can be easily applied to modest-resource languages as they involve simple steps such as scraping news articles and Wikipedia infoboxes, light cleaning, and pivoting through machine translation data. To the best of our knowledge, the IndicNLG Benchmark is the first NLG benchmark for Indic languages and the most diverse multilingual NLG dataset, with approximately 8M examples across 5 tasks and 11 languages. The datasets and models will be publicly available.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>main</td>\n",
       "      <td>Investigating the Robustness of Natural Language Generation from Logical Forms via Counterfactual Samples</td>\n",
       "      <td>The aim of Logic2Text is to generate controllable and faithful texts conditioned on tables and logical forms, which not only requires a deep understanding of the tables and logical forms, but also warrants symbolic reasoning over the tables according to the logical forms. State-of-the-art methods based on pre-trained models have achieved remarkable performance on the standard test dataset. However, we question whether these methods really learn how to perform logical reasoning, rather than just relying on the spurious correlations between the headers of the tables and operators of the logical form. To verify this hypothesis, we manually construct a set of counterfactual samples, which modify the original logical forms to generate counterfactual logical forms with rare co-occurred headers and operators and corresponding counterfactual references. SOTA methods give much worse results on these counterfactual samples compared with the results on the original test dataset, which verifies our hypothesis. To deal with this problem, we firstly analyze this bias from a causal perspective, based on which we propose two approaches to reduce the model’s reliance on the shortcut. The first one incorporates the hierarchical structure of the logical forms into the model. The second one exploits automatically generated counterfactual data for training. Automatic and manual experimental results on the original test dataset and counterfactual dataset show that our method is effective to alleviate the spurious correlation. Our work points out the weakness of current methods and takes a further step toward developing Logic2Text models with real logical reasoning ability.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>2484</td>\n",
       "      <td>2484</td>\n",
       "      <td>2484</td>\n",
       "      <td>2484</td>\n",
       "      <td>main</td>\n",
       "      <td>PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation</td>\n",
       "      <td>Logical table-to-text generation is a task that involves generating logically faithful sentences from tables, which requires models to derive logical-level facts from table records via logical inference. It raises a new challenge on the logical-level content planning of table-to-text models. However, directly learning the logical inference knowledge from table-text pairs is very difficult for neural models because of the ambiguity of natural language and the scarcity of parallel data. Hence even large-scale pre-trained language models present low logical fidelity on logical table-to-text. In this work, we propose a Pretrained Logical Form Generator (PLOG) framework to improve generation fidelity. Specifically, PLOG is first pretrained on a table-to-logical-form generation (table-to-logic) task, then finetuned on downstream table-to-text tasks. The logical forms are formally defined with unambiguous semantics. Hence we can collect a large amount of accurate logical forms from tables without human annotation. In addition, PLOG can learn logical inference from table-logic pairs much more reliably than from table-text pairs. To evaluate our model, we further collect a controlled logical table-to-text dataset CONTLOG based on an existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms strong baselines by a large margin on the logical fidelity, demonstrating the effectiveness of table-to-logic pretraining.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>2559</td>\n",
       "      <td>2559</td>\n",
       "      <td>2559</td>\n",
       "      <td>2559</td>\n",
       "      <td>main</td>\n",
       "      <td>DEER: Descriptive Knowledge Graph for Explaining Entity Relationships</td>\n",
       "      <td>We propose DEER (Descriptive Knowledge Graph for Explaining Entity Relationships) - an open and informative form of modeling entity relationships. In DEER, relationships between entities are represented by free-text relation descriptions. For instance, the relationship between entities of machine learning and algorithm can be represented as “Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.” To construct DEER, we propose a self-supervised learning method to extract relation descriptions with the analysis of dependency patterns and generate relation descriptions with a transformer-based relation description synthesizing model, where no human labeling is required. Experiments demonstrate that our system can extract and generate high-quality relation descriptions for explaining entity relationships. The results suggest that we can build an open and informative knowledge graph without human annotation.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>2573</td>\n",
       "      <td>2573</td>\n",
       "      <td>2573</td>\n",
       "      <td>2573</td>\n",
       "      <td>main</td>\n",
       "      <td>Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples</td>\n",
       "      <td>Knowledge-enhanced language representation learning has shown promising results across various knowledge-intensive NLP tasks. However, prior methods are limited in efficient utilization of multilingual knowledge graph (KG) data for language model (LM) pretraining. They often train LMs with KGs in indirect ways, relying on extra entity/relation embeddings to facilitate knowledge injection. In this work, we explore methods to make better use of the multilingual annotation and language agnostic property of KG triples, and present novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples. We first generate a large amount of multilingual synthetic sentences using the Wikidata KG triples. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to enable the LMs to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual tasks, including named entity recognition (NER), factual knowledge retrieval, relation classification, and a newly designed logical reasoning task.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>2575</td>\n",
       "      <td>2575</td>\n",
       "      <td>2575</td>\n",
       "      <td>2575</td>\n",
       "      <td>main</td>\n",
       "      <td>R2D2: Robust Data-to-Text with Replacement Detection</td>\n",
       "      <td>Unfaithful text generation is a common problem for text generation systems. In the case of Data-to-Text (D2T) systems, the factuality of the generated text is particularly crucial for any real-world applications. We introduce R2D2, a training framework that addresses unfaithful Data-to-Text generation by training a system both as a generator and a faithfulness discriminator with additional replacement detection and unlikelihood learning tasks. To facilitate such training, we propose two methods for sampling unfaithful sentences. We argue that the poor entity retrieval capability of D2T systems is one of the primary sources of unfaithfulness, so in addition to the existing metrics, we further propose named entity based metrics to evaluate the fidelity of D2T generations. Our experimental results show that R2D2 systems could effectively mitigate the unfaithful text generation, and they achieve new state-of-theart results on FeTaQA, LogicNLG, and ToTTo, all with significant improvements.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>2580</td>\n",
       "      <td>2580</td>\n",
       "      <td>2580</td>\n",
       "      <td>2580</td>\n",
       "      <td>main</td>\n",
       "      <td>PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance</td>\n",
       "      <td>To facilitate conversational question answering (CQA) over hybrid contexts in finance, we present a new dataset, named PACIFIC. Compared with existing CQA datasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical reasoning, and (iii) hybrid context of tables and text. A new task is defined accordingly to study Proactive Conversational Question Answering (PCQA), which combines clarification question generation and CQA. In addition, we propose a novel method, namely UniPCQA, to adapt a hybrid format of input and output content in PCQA into the Seq2Seq problem, including the reformulation of the numerical reasoning process as code generation. UniPCQA performs multi-task learning over all sub-tasks in PCQA and incorporates a simple ensemble strategy to alleviate the error propagation issue in the multi-task learning by cross-validating top-k sampled Seq2Seq outputs. We benchmark the PACIFIC dataset with extensive baselines and provide comprehensive evaluations on each sub-task of PCQA.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>2619</td>\n",
       "      <td>2619</td>\n",
       "      <td>2619</td>\n",
       "      <td>2619</td>\n",
       "      <td>main</td>\n",
       "      <td>UniRPG: Unified Discrete Reasoning over Table and Text as Program Generation</td>\n",
       "      <td>Question answering requiring discrete reasoning, e.g., arithmetic computing, comparison, and counting, over knowledge is a challenging task.In this paper, we propose UniRPG, a semantic-parsing-based approach advanced in interpretability and scalability, to perform Unified discrete Reasoning over heterogeneous knowledge resources, i.e., table and text, as Program Generation. Concretely, UniRPG consists of a neural programmer and a symbolic program executor,where a program is the composition of a set of pre-defined general atomic and higher-order operations and arguments extracted from table and text.First, the programmer parses a question into a program by generating operations and copying arguments, and then, the executor derives answers from table and text based on the program.To alleviate the costly program annotation issue, we design a distant supervision approach for programmer learning, where pseudo programs are automatically constructed without annotated derivations.Extensive experiments on the TAT-QA dataset show that UniRPG achieves tremendous improvements and enhances interpretability and scalability compared with previous state-of-the-art methods, even without derivation annotation.Moreover, it achieves promising performance on the textual dataset DROP without derivation annotation.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>2673</td>\n",
       "      <td>2673</td>\n",
       "      <td>2673</td>\n",
       "      <td>2673</td>\n",
       "      <td>main</td>\n",
       "      <td>Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach</td>\n",
       "      <td>Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables. Large-scale pretrained language models sound like a promising solution to tackle such issues. However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored. Besides, another challenge of integrating the deliberation mechanism into the text-to-text pretrained model for solving the table-to-text task remains seldom studied. In this paper, to implement the table-to-text generation with pretrained language model, we propose a table structure understanding and text deliberating approach, namely TASD. To be specific, we devise a three-layered multi-head attention network to realize the table-structureaware text generation model with the help of the pretrained language model. Furthermore, a multi-pass decoder framework is adopted to enhance the capability of polishing generated text for table descriptions. The empirical studies, as well as human evaluation, on two public datasets, validate that our approach can generate faithful and fluent descriptive texts for different types of tables.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>2716</td>\n",
       "      <td>2716</td>\n",
       "      <td>2716</td>\n",
       "      <td>2716</td>\n",
       "      <td>main</td>\n",
       "      <td>Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database</td>\n",
       "      <td>Parsing natural language questions into executable logical forms is a useful and interpretable way to perform question answering on structured data such as knowledge bases (KB) or databases (DB). However, existing approaches on semantic parsing cannot adapt to both modalities, as they suffer from the exponential growth of the logical form candidates and can hardly generalize to unseen data.In this work, we propose Uni-Parser, a unified semantic parser for question answering (QA) on both KB and DB. We define the primitive (relation and entity in KB, and table name, column name and cell value in DB) as the essential element in our framework. The number of primitives grows only at a linear rate to the number of retrieved relations in KB and DB, preventing us from exponential logic form candidates. We leverage the generator to predict final logical forms by altering and composing top-ranked primitives with different operations (e.g. select, where, count). With sufficiently pruned search space by a contrastive primitive ranker, the generator is empowered to capture the composition of primitives enhancing its generalization ability. We achieve competitive results on multiple KB and DB QA benchmarks with more efficiency, especially in the compositional and zero-shot settings.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'database-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>database-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>2726</td>\n",
       "      <td>2726</td>\n",
       "      <td>2726</td>\n",
       "      <td>2726</td>\n",
       "      <td>main</td>\n",
       "      <td>ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples</td>\n",
       "      <td>Reasoning over tabular data requires both table structure understanding and a broad set of table reasoning skills. Current models with table-specific architectures and pre-training methods perform well on understanding table structures, but they still struggle with tasks that require various table reasoning skills. In this work, we develop ReasTAP to show that high-level table reasoning skills can be injected into models during pre-training without a complex table-specific architecture design. We define 7 table reasoning skills, such as numerical operation, temporal comparison, and conjunction. Each reasoning skill is associated with one example generator, which synthesizes questions over semi-structured tables according to the sampled templates. We model the table pre-training task as a sequence generation task and pre-train ReasTAP to generate precise answers of the synthetic examples. ReasTAP is evaluated on four benchmarks covering three downstream tasks including 1) WikiSQL-Weak and WikiTQ for Table Question Answering, 2) TabFact for Table Fact Verification, and 3) LogicNLG for Faithful Table-to-Text Generation. Experimental results demonstrate that ReasTAP achieves new state-of-the-art results on all of them and delivers a significant improvement under low-resource setting. Our code is publicly available at https://github.com/Yale-LILY/ReasTAP.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>2786</td>\n",
       "      <td>2786</td>\n",
       "      <td>2786</td>\n",
       "      <td>2786</td>\n",
       "      <td>main</td>\n",
       "      <td>VisToT: Vision-Augmented Table-to-Text Generation</td>\n",
       "      <td>Table-to-text generation has been widely studied in the Natural Language Processing community in the recent years. We give a new perspective to this problem by incorporating signals from both tables as well as associated images to generate relevant text. While tables contain a structured list of facts, images are a rich source of unstructured visual information. For example, in the tourism domain, images can be used to infer knowledge such as the type of landmark (e.g., church), its architecture (e.g., Ancient Roman), and composition (e.g., white marble). Therefore, in this paper, we introduce the novel task of Vision-augmented Table-To-Text Generation (VisToT, defined as follows: given a table and an associated image, produce a descriptive sentence conditioned on the multimodal input. For the task, we present a novel multimodal table-to-text dataset, WikiLandmarks, covering 73,084 unique world landmarks. Further, we also present a competitive architecture, namely, VT3 that generates accurate sentences conditioned on the image and table pairs. Through extensive analyses and experiments, we show that visual cues from images are helpful in (i) inferring missing information from incomplete or sparse tables, and (ii) strengthening the importance of useful information from noisy tables for natural language generation. We make the code and data publicly available.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status': true, 'class': 'Table-to-text generation'}\\n```</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>2951</td>\n",
       "      <td>2951</td>\n",
       "      <td>2951</td>\n",
       "      <td>2951</td>\n",
       "      <td>main</td>\n",
       "      <td>Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation</td>\n",
       "      <td>Knowledge-grounded dialogue systems are challenging to build due to the lack of training data and heterogeneous knowledge sources. Existing systems perform poorly on unseen topics due to limited topics covered in the training data. In addition, it is challenging to generalize to the domains that require different types of knowledge sources. To address the above challenges, we present PLUG, a language model that homogenizes different knowledge sources to a unified knowledge representation for knowledge-grounded dialogue generation tasks. We first retrieve relevant information from heterogeneous knowledge sources (e.g., wiki, dictionary, or knowledge graph); Then the retrieved knowledge is transformed into text and concatenated with dialogue history to feed into the language model for generating responses. PLUG is pre-trained on a large-scale knowledge-grounded dialogue corpus. The empirical evaluation on two benchmarks shows that PLUG generalizes well across different knowledge-grounded dialogue tasks. It achieves comparable performance with state-of-the-art methods in the fully-supervised setting and significantly outperforms other approaches in zero-shot and few-shot settings.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>2993</td>\n",
       "      <td>2993</td>\n",
       "      <td>2993</td>\n",
       "      <td>2993</td>\n",
       "      <td>main</td>\n",
       "      <td>NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics</td>\n",
       "      <td>The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A* search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>3004</td>\n",
       "      <td>3004</td>\n",
       "      <td>3004</td>\n",
       "      <td>3004</td>\n",
       "      <td>main</td>\n",
       "      <td>OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering</td>\n",
       "      <td>The information in tables can be an important complement to text, making table-based question answering (QA) systems of great value. The intrinsic complexity of handling tables often adds an extra burden to both model design and data annotation. In this paper, we aim to develop a simple table-based QA model with minimal annotation effort. Motivated by the fact that table-based QA requires both alignment between questions and tables and the ability to perform complicated reasoning over multiple table elements, we propose an omnivorous pretraining approach that consumes both natural and synthetic data to endow models with these respective abilities. Specifically, given freely available tables, we leverage retrieval to pair them with relevant natural sentences for mask-based pretraining, and synthesize NL questions by converting SQL sampled from tables for pretraining with a QA loss. We perform extensive experiments in both few-shot and full settings, and the results clearly demonstrate the superiority of our model OmniTab, with the best multitasking approach achieving an absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively, also establishing a new state-of-the-art on WikiTableQuestions. Detailed ablations and analyses reveal different characteristics of natural and synthetic data, shedding light on future directions in omnivorous pretraining.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>3019</td>\n",
       "      <td>3019</td>\n",
       "      <td>3019</td>\n",
       "      <td>3019</td>\n",
       "      <td>main</td>\n",
       "      <td>CoSe-Co: Text Conditioned Generative CommonSense Contextualizer</td>\n",
       "      <td>Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have leveraged structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to improve the scale at which knowledge can be obtained. However, training on symbolic KG entities limits their applicability in tasks involving natural language text where they ignore overall context. To mitigate this, we propose a CommonSense Contextualizer (CoSe-Co) conditioned on sentences as input to make it generically usable in tasks for generating knowledge relevant to the overall context of input text. To train CoSe-Co, we propose a novel dataset comprising of sentence and commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and contain novel entities not present in the underlying KG. We augment generated knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets. We also demonstrate its applicability in improving performance of a baseline model for paraphrase generation task.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>3049</td>\n",
       "      <td>3049</td>\n",
       "      <td>3049</td>\n",
       "      <td>3049</td>\n",
       "      <td>main</td>\n",
       "      <td>SKILL: Structured Knowledge Infusion for Large Language Models</td>\n",
       "      <td>Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size KG, WikiMovies, we saw 3x improvement of exact match score on MetaQA task. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Knowledge graph-to-text generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>3307</td>\n",
       "      <td>3307</td>\n",
       "      <td>3307</td>\n",
       "      <td>3307</td>\n",
       "      <td>main</td>\n",
       "      <td>Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning</td>\n",
       "      <td>Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly, we propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column, and it differentiates between relevant cells and irrelevant cells from the structural perspective. Our framework also modifies the positional encoding mechanism to preserve the relative position of tokens in the same cell but enforce position invariance among different cells. Our technology is free to be plugged into existing table-to-text generation models, and has improved T5-based models to offer better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo, we preserve promising performance, while previous SOTA systems, even with transformation-based data augmentation, have seen significant performance drops.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Table-to-text generation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0 paper_type  \\\n",
       "211            211           211           211         211      short   \n",
       "248            248           248           248         248       long   \n",
       "290            290           290           290         290       long   \n",
       "359            359           359           359         359       long   \n",
       "542            542           542           542         542       long   \n",
       "557            557           557           557         557       long   \n",
       "630            630           630           630         630       long   \n",
       "697            697           697           697         697       long   \n",
       "711            711           711           711         711      short   \n",
       "751            751           751           751         751      short   \n",
       "824            824           824           824         824       main   \n",
       "835            835           835           835         835       main   \n",
       "960            960           960           960         960       main   \n",
       "968            968           968           968         968       main   \n",
       "990            990           990           990         990       main   \n",
       "1030          1030          1030          1030        1030       main   \n",
       "1101          1101          1101          1101        1101       main   \n",
       "1138          1138          1138          1138        1138       main   \n",
       "1196          1196          1196          1196        1196       main   \n",
       "1213          1213          1213          1213        1213       main   \n",
       "1236          1236          1236          1236        1236       main   \n",
       "1292          1292          1292          1292        1292       main   \n",
       "1327          1327          1327          1327        1327       main   \n",
       "1414          1414          1414          1414        1414       main   \n",
       "1453          1453          1453          1453        1453       main   \n",
       "1506          1506          1506          1506        1506       main   \n",
       "1547          1547          1547          1547        1547       main   \n",
       "1562          1562          1562          1562        1562       main   \n",
       "1601          1601          1601          1601        1601       main   \n",
       "1834          1834          1834          1834        1834       main   \n",
       "1961          1961          1961          1961        1961       main   \n",
       "1978          1978          1978          1978        1978       main   \n",
       "2046          2046          2046          2046        2046       main   \n",
       "2067          2067          2067          2067        2067       main   \n",
       "2150          2150          2150          2150        2150       main   \n",
       "2233          2233          2233          2233        2233       main   \n",
       "2282          2282          2282          2282        2282       main   \n",
       "2385          2385          2385          2385        2385       main   \n",
       "2392          2392          2392          2392        2392       main   \n",
       "2432          2432          2432          2432        2432       main   \n",
       "2471          2471          2471          2471        2471       main   \n",
       "2481          2481          2481          2481        2481       main   \n",
       "2484          2484          2484          2484        2484       main   \n",
       "2559          2559          2559          2559        2559       main   \n",
       "2573          2573          2573          2573        2573       main   \n",
       "2575          2575          2575          2575        2575       main   \n",
       "2580          2580          2580          2580        2580       main   \n",
       "2619          2619          2619          2619        2619       main   \n",
       "2673          2673          2673          2673        2673       main   \n",
       "2716          2716          2716          2716        2716       main   \n",
       "2726          2726          2726          2726        2726       main   \n",
       "2786          2786          2786          2786        2786       main   \n",
       "2951          2951          2951          2951        2951       main   \n",
       "2993          2993          2993          2993        2993       main   \n",
       "3004          3004          3004          3004        3004       main   \n",
       "3019          3019          3019          3019        3019       main   \n",
       "3049          3049          3049          3049        3049       main   \n",
       "3307          3307          3307          3307        3307       main   \n",
       "\n",
       "                                                                                                                                   title  \\\n",
       "211                                    A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation   \n",
       "248                                                                     Rethinking Tabular Data Understanding with Large Language Models   \n",
       "290                E5: Zero-shot Hierarchical Table Analysis using Augmented LLMs via Explain, Extract, Execute, Exhibit and Extrapolate   \n",
       "359                                                FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering   \n",
       "542                                                      TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition   \n",
       "557                                                                          TableLlama: Towards Open Large Generalist Models for Tables   \n",
       "630                                      Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in ToTTo   \n",
       "697                    Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning   \n",
       "711                                           Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?   \n",
       "751                    Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata   \n",
       "824                                       LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control   \n",
       "835                        Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters   \n",
       "960                                                     Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models   \n",
       "968                                                         Semantic Parsing for Conversational Question Answering over Knowledge Graphs   \n",
       "990                                      The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents   \n",
       "1030                                           KGVL-BART: Knowledge Graph Augmented Visual Language BART for Radiology Report Generation   \n",
       "1101                                                                 Knowledge Graph Compression Enhances Diverse Commonsense Generation   \n",
       "1138                                                                               QTSumm: Query-Focused Summarization over Tabular Data   \n",
       "1196                                                         CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data   \n",
       "1213                                                                   TempTabQA: Temporal Question Answering for Semi-Structured Tables   \n",
       "1236                                                     Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation   \n",
       "1292      ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph   \n",
       "1327                               Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation   \n",
       "1414                                                      Controllable Contrastive Generation for Multilingual Biomedical Entity Linking   \n",
       "1453                                                                            ReTAG: Reasoning Aware Table to Analytic Text Generation   \n",
       "1506  HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies   \n",
       "1547                             SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables   \n",
       "1562                                                               Enhancing Biomedical Lay Summarisation with External Knowledge Graphs   \n",
       "1601                               Structure-aware Knowledge Graph-to-text Generation with Planning Selection and Similarity Distinction   \n",
       "1834                               Hallucination Mitigation in Natural Language Generation from Large-Scale Open-Domain Knowledge Graphs   \n",
       "1961                                                      API-Assisted Code Generation for Question Answering on Varied Table Structures   \n",
       "1978                                                                                     Large Language Models are Complex Table Parsers   \n",
       "2046                                           Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue   \n",
       "2067                                                                       ToolWriter: Question Specific Tool Synthesis for Tabular Data   \n",
       "2150                             UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models   \n",
       "2233                                     Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs   \n",
       "2282                                             PRINCE: Prefix-Masked Decoding for Knowledge Enhanced Sequence-to-Sequence Pre-Training   \n",
       "2385                                                        CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation   \n",
       "2392                                       DSM: Question Generation over Knowledge Base via Modeling Diverse Subgraphs with Meta-learner   \n",
       "2432                                                             Self-supervised Graph Masking Pre-training for Graph-to-Text Generation   \n",
       "2471                                                  IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic Languages   \n",
       "2481                           Investigating the Robustness of Natural Language Generation from Logical Forms via Counterfactual Samples   \n",
       "2484                                                               PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation   \n",
       "2559                                                               DEER: Descriptive Knowledge Graph for Explaining Entity Relationships   \n",
       "2573                                                   Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples   \n",
       "2575                                                                                R2D2: Robust Data-to-Text with Replacement Detection   \n",
       "2580                               PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance   \n",
       "2619                                                        UniRPG: Unified Discrete Reasoning over Table and Text as Program Generation   \n",
       "2673     Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach   \n",
       "2716                                           Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database   \n",
       "2726                                      ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples   \n",
       "2786                                                                                   VisToT: Vision-Augmented Table-to-Text Generation   \n",
       "2951                                                      Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation   \n",
       "2993                                                  NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics   \n",
       "3004                                    OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering   \n",
       "3019                                                                     CoSe-Co: Text Conditioned Generative CommonSense Contextualizer   \n",
       "3049                                                                      SKILL: Structured Knowledge Infusion for Large Language Models   \n",
       "3307                                             Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           abstract  \\\n",
       "211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models’ performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance.   \n",
       "248                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WikiTableQuestions, representing a substantial advancement over previous existing table processing paradigms of LLMs.   \n",
       "290                                                                                                                                                                                                                                                                                                                                                                                                          Analyzing large hierarchical tables with multi-level headers presents challenges due to their complex structure, implicit semantics, and calculation relationships. While recent advancements in large language models (LLMs) have shown promise in flat table analysis, their application to hierarchical tables is constrained by the reliance on manually curated exemplars and the model’s token capacity limitations. Addressing these challenges, we introduce a novel code-augmented LLM-based framework, E5, for zero-shot hierarchical table question answering. This approach encompasses self-explaining the table’s hierarchical structures, code generation to extract relevant information and apply operations, external code execution to prevent hallucinations, and leveraging LLMs’ reasoning for final answer derivation. Empirical results indicate that our method, based on GPT-4, outperforms state-of-the-art fine-tuning methods with a 44.38 Exact Match improvement. Furthermore, we present F3, an adaptive algorithm designed for token-limited scenarios, effectively condensing large tables while maintaining useful information. Our experiments prove its efficiency, enabling the processing of large tables even with models having limited context lengths. The code is available at https://github.com/zzh-SJTU/E5-Hierarchical-Table-Analysis.   \n",
       "359                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Table Question Answering (TQA) aims at composing an answer to a question based on tabular data. While prior research has shown that TQA models lack robustness, understanding the underlying cause and nature of this issue remains predominantly unclear, posing a significant obstacle to the development of robust TQA systems. In this paper, we formalize three major desiderata for a fine-grained evaluation of robustness of TQA systems. They should (i) answer questions regardless of alterations in table structure, (ii) base their responses on the content of relevant cells rather than on biases, and (iii) demonstrate robust numerical reasoning capabilities. To investigate these aspects, we create and publish a novel TQA evaluation benchmark in English. Our extensive experimental analysis reveals that none of the examined state-of-the-art TQA systems consistently excels in these three aspects. Our benchmark is a crucial instrument for monitoring the behavior of TQA systems and paves the way for the development of robust TQA systems. We release our benchmark publicly.   \n",
       "542                                                                                                                                                                                                                                                                                                                                                                                                    Table reasoning is a challenging task that requires understanding both natural language questions and structured tabular data. Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation, but they often struggle with large tables due to their limited input length. In this paper, we propose TabSQLify, a novel method that leverages text-to-SQL generation to decompose tables into smaller and relevant sub-tables, containing only essential information for answering questions or verifying statements, before performing the reasoning task. In our comprehensive evaluation on four challenging datasets, our approach demonstrates comparable or superior performance compared to prevailing methods reliant on full tables as input. Moreover, our method can reduce the input context length significantly, making it more scalable and efficient for large-scale table reasoning applications. Our method performs remarkably well on the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the TabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass other LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can reduce the table size significantly alleviating the computational load on LLMs when handling large tables without compromising performance.   \n",
       "557                                                                                                                                                                                                                                                                                                                                                                                                                                        Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 5-44 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model’s generalizability. We open-source our dataset and trained model to boost future work on developing open generalist models for tables.   \n",
       "630                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Neural Table-to-Text models tend to hallucinate, producing texts that contain factual errors. We investigate whether such errors in the output can be traced back to problems with the input. We manually annotated 1,837 texts generated by multiple models in the politics domain of the ToTTo dataset. We identify the input problems that are responsible for many output errors and show that fixing these inputs reduces factual errors by between 52% and 76% (depending on the model). In addition, we observe that models struggle in processing tabular inputs that are structured in a non-standard way, particularly when the input lacks distinct row and column values or when the column headers are not correctly mapped to corresponding values.   \n",
       "697                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             An important open question in the use of large language models for knowledge-intensive tasks is how to effectively integrate knowledge from three sources: the model’s parametric memory, external structured knowledge, and external unstructured knowledge. Most existing prompting methods either rely on one or two of these sources, or require repeatedly invoking large language models to generate similar or identical content. In this work, we overcome these limitations by introducing a novel semi-structured prompting approach that seamlessly integrates the model’s parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs. Experimental results on open-domain multi-hop question answering datasets demonstrate that our prompting method significantly surpasses existing techniques, even exceeding those that require fine-tuning.   \n",
       "711                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging. Our study assesses LLMs’ proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance. We unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats. Our proposed FormatCoT aids in crafting format-specific instructions from the intended outputs to populate this benchmark. Addressing the gap in task-centered evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM performance. Our experiments show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial performance gains, outshining its LLM counterparts across most measures. In-depth error analysis and creating an ability map across six dimensions, coverage, formatting, reasoning, comprehension, pragmatics, and hallucination, highlight areas for future enhancements and suggest forthcoming research trajectories. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.   \n",
       "751                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The Directed Acyclic Transformer is a fast non-autoregressive (NAR) model that performs well in Neural Machine Translation. Two issues prevent its application to general Natural Language Generation (NLG) tasks: frequent Out-Of-Vocabulary (OOV) errors and the inability to faithfully generate entity names. We introduce Control-DAG, a constrained decoding algorithm for our Directed Acyclic T5 (DA-T5) model which offers lexical, vocabulary and length control. We show that Control-DAG significantly enhances DA-T5 on the Schema Guided Dialogue and the DART datasets, establishing strong NAR results for Task-Oriented Dialogue and Data-to-Text NLG.   \n",
       "824                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Logical Table-to-Text (LT2T) generation is tasked with generating logically faithful sentences from tables. There currently exists two challenges in the field: 1) Faithfulness: how to generate sentences that are factually correct given the table content; 2) Diversity: how to generate multiple sentences that offer different perspectives on the table. This work proposes LoFT, which utilizes logic forms as fact verifiers and content planners to control LT2T generation. Experimental results on the LogicNLG dataset demonstrate that LoFT is the first model that addresses unfaithfulness and lack of diversity issues simultaneously. Our code is publicly available at https://github.com/Yale-LILY/LoFT.   \n",
       "835                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Text generation from Abstract Meaning Representation (AMR) has substantially benefited from the popularized Pretrained Language Models (PLMs). Myriad approaches have linearized the input graph as a sequence of tokens to fit the PLM tokenization requirements. Nevertheless, this transformation jeopardizes the structural integrity of the graph and is therefore detrimental to its resulting representation. To overcome this issue, Ribeiro et al. (2021b) have recently proposed StructAdapt, a structure-aware adapter which injects the input graph connectivity within PLMs using Graph Neural Networks (GNNs). In this paper, we investigate the influence of Relative Position Embeddings (RPE) on AMR-to-Text, and, in parallel, we examine the robustness of StructAdapt. Through ablation studies, graph attack and link prediction, we reveal that RPE might be partially encoding input graphs. We suggest further research regarding the role of RPE will provide valuable insights for Graph-to-Text generation.   \n",
       "960                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Pretrained language models (PLMs) for data-to-text (D2T) generation can use human-readable data labels such as column headings, keys, or relation names to generalize to out-of-domain examples. However, the models are well-known in producing semantically inaccurate outputs if these labels are ambiguous or incomplete, which is often the case in D2T datasets. In this paper, we expose this issue on the task of descibing a relation between two entities. For our experiments, we collect a novel dataset for verbalizing a diverse set of 1,522 unique relations from three large-scale knowledge graphs (Wikidata, DBPedia, YAGO). We find that although PLMs for D2T generation expectedly fail on unclear cases, models trained with a large variety of relation labels are surprisingly robust in verbalizing novel, unseen relations. We argue that using data with a diverse set of clear and meaningful labels is key to training D2T generation systems capable of generalizing to novel domains.   \n",
       "968                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In this paper, we are interested in developing semantic parsers which understand natural language questions embedded in a conversation with a user and ground them to formal queries over definitions in a general purpose knowledge graph (KG) with very large vocabularies (covering thousands of concept names and relations, and millions of entities). To this end, we develop a dataset where user questions are annotated with Sparql parses and system answers correspond to execution results thereof. We present two different semantic parsing approaches and highlight the challenges of the task: dealing with large vocabularies, modelling conversation context, predicting queries with multiple entities, and generalising to new questions at test time. We hope our dataset will serve as useful testbed for the development of conversational semantic parsers. Our dataset and models are released at https://github.com/EdinburghNLP/SPICE.   \n",
       "990                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation turns between agents working at Statistics Canada and online users looking for published data tables. The conversations stem from genuine intents, are held in English or French, and lead to agents retrieving one of over 5000 complex data tables. Based on this dataset, we propose two tasks: (1) automatic retrieval of relevant tables based on a on-going conversation, and (2) automatic generation of appropriate agent responses at each turn. We investigate the difficulty of each task by establishing strong baselines. Our experiments on a temporal data split reveal that all models struggle to generalize to future conversations, as we observe a significant drop in performance across both tasks when we move from the validation to the test set. In addition, we find that response generation models struggle to decide when to return a table. Considering that the tasks pose significant challenges to existing models, we encourage the community to develop models for our task, which can be directly used to help knowledge workers find relevant tables for live chat users.   \n",
       "1030                                   Timely generation of radiology reports and diagnoses is a challenge worldwide due to the enormous number of cases and shortage of radiology specialists. In this paper, we propose a Knowledge Graph Augmented Vision Language BART (KGVL-BART) model that takes as input two chest X-ray images- one frontal and the other lateral- along with tags which are diagnostic keywords, and outputs a report with the patient-specific findings. Our system development effort is divided into 3 stages: i) construction of the Chest X-ray KG (referred to as chestX-KG), ii) image feature extraction, and iii) training a KGVL-BART model using the visual, text, and KG data. The dataset we use is the well-known Indiana University Chest X-ray reports with the train, validation, and test split of 3025 instances, 300 instances, and 500 instances respectively. We construct a Chest X-Ray knowledge graph from these reports by extracting entity1-relation-entity2 triples; the triples get extracted by a rule-based tool of our own. Constructed KG is verified by two experienced radiologists (with experience of 30 years and 8 years, respectively). We demonstrate that our model- KGVL-BART- outperforms State-of-the-Art transformer-based models on standard NLG scoring metrics. We also include a qualitative evaluation of our system by experienced radiologist (with experience of 30 years) on the test data, which showed that 73% of the reports generated were fully correct, only 5.5% are completely wrong and 21.5% have important missing details though overall correct. To the best of our knowledge, ours is the first system to make use of multi-modality and domain knowledge to generate X-ray reports automatically.   \n",
       "1101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Generating commonsense explanations requires reasoning about commonsense knowledge beyond what is explicitly mentioned in the context. Existing models use commonsense knowledge graphs such as ConceptNet to extract a subgraph of relevant knowledge pertaining to concepts in the input. However, due to the large coverage and, consequently, vast scale of ConceptNet, the extracted subgraphs may contain loosely related, redundant and irrelevant information, which can introduce noise into the model. We propose to address this by applying a differentiable graph compression algorithm that focuses on the relevant knowledge for the task. The compressed subgraphs yield considerably more diverse outputs when incorporated into models for the tasks of generating commonsense and abductive explanations. Moreover, our model achieves better quality-diversity tradeoff than a large language model with 100 times the number of parameters. Our generic approach can be applied to additional NLP tasks that can benefit from incorporating external knowledge.   \n",
       "1138                                                                                                                                                                                                                                                                                                                                                                                                                                                People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users’ information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.   \n",
       "1196                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Large language models (LLMs) show powerful reasoning abilities on various text-based tasks. However, their reasoning capability on structured data such as tables has not been systematically explored. In this work, we first establish a comprehensive taxonomy of reasoning and operation types for tabular data analysis. Then, we construct a complex reasoning QA dataset over tabular data, named CRT-QA dataset (Complex Reasoning QA over Tabular data), with the following unique features: (1) it is the first Table QA dataset with multi-step operation and informal reasoning; (2) it contains fine-grained annotations on questions’ directness, composition types of sub-questions, and human reasoning paths which can be used to conduct a thorough investigation on LLMs’ reasoning ability; (3) it contains a collection of unanswerable and indeterminate questions that commonly arise in real-world situations. We further introduce an efficient and effective tool-augmented method, named ARC (Auto-exemplar-guided Reasoning with Code), to use external tools such as Pandas to solve table reasoning tasks without handcrafted demonstrations. The experiment results show that CRT-QA presents a strong challenge for baseline methods and ARC achieves the best result.   \n",
       "1213                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on semi-structured tables. We present a dataset, TEMPTABQA, which comprises 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning more than 90 distinct domains. Using this dataset, we evaluate several state-of-the-art models for temporal reasoning. We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points. Given these results, our dataset has the potential to serve as a challenging benchmark to improve the temporal reasoning capabilities of NLP models.   \n",
       "1236                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special “text critic” classifier, which guides the generation by assessing the match between the input data and the text generated so far. Our method does not need any changes to the underlying LM’s architecture or training procedure and can thus be combined with any model and decoding operating on word probabilities. The critic does not need any additional training data, using the base LM’s training data and synthetic negative examples. Our experimental results show that our method improves over the baseline on the WebNLG and OpenDialKG benchmarks.   \n",
       "1292                                                                                                                                                                                                                                                                                                                                                                                                                                                            Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph (KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model (PLM) to model the question, and a graph neural network (GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin, even with fewer updated parameters and less training data. Our codes and data are publicly available at https://github.com/RUCAIBox/ReasoningLM.   \n",
       "1327                                                                                                                                                               The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.   \n",
       "1414                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Multilingual biomedical entity linking (MBEL) aims to map language-specific mentions in the biomedical text to standardized concepts in a multilingual knowledge base (KB) such as Unified Medical Language System (UMLS). In this paper, we propose Con2GEN, a prompt-based controllable contrastive generation framework for MBEL, which summarizes multidimensional information of the UMLS concept mentioned in biomedical text into a natural sentence following a predefined template. Instead of tackling the MBEL problem with a discriminative classifier, we formulate it as a sequence-to-sequence generation task, which better exploits the shared dependencies between source mentions and target entities. Moreover, Con2GEN matches against UMLS concepts in as many languages and types as possible, hence facilitating cross-information disambiguation. Extensive experiments show that our model achieves promising performance improvements compared with several state-of-the-art techniques on the XL-BEL and the Mantra GSC datasets spanning 12 typologically diverse languages.   \n",
       "1453                                                                                                                                                                                                  The task of table summarization involves generating text that both succinctly and accurately represents the table or a specific set of highlighted cells within a table. While significant progress has been made in table to text generation techniques, models still mostly generate descriptive summaries, which reiterates the information contained within the table in sentences. Through analysis of popular table to text benchmarks (ToTTo (Parikh et al., 2020 and InfoTabs (Gupta et al., 2020) we observe that in order to generate the ideal summary, multiple types of reasoning is needed coupled with access to knowledge beyond the scope of the table. To address this gap, we propose ReTAG, a table and reasoning aware model that uses vector-quantization to infuse different types of analytical reasoning into the output. ReTAG achieves 2.2%, 2.9% improvement on the PARENT metric in the relevant slice of ToTTo and InfoTabs for the table to text generation task over state of the art baselines. Through human evaluation, we observe that output from ReTAG is upto 12% more faithful and analytical compared to a strong table-aware model. To the best of our knowledge, ReTAG is the first model that can controllably use multiple reasoning methods within a structure-aware sequence to sequence model to surpass state of the art performance in multiple table to text tasks. We extend (and open source 35.6K analytical, 55.9k descriptive instances) the ToTTo, InfoTabs datasets with the reasoning categories used in each reference sentences.   \n",
       "1506  A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external models such as gpt-35-turbo. We propose a cooperative game dubbed “HiddenTables” as a potential resolution to this challenge. In essence, “HiddenTables” is played between the code-generating LLM “Solver” and the “Oracle” which evaluates the ability of the LLM agents to solve TableQA tasks. This game is based on natural language schemas and importantly, ensures the security of the underlying data. We provide evidential experiments on a diverse set of tables that demonstrate an LLM’s collective inability to generalize and perform on complex queries, handle compositional dependencies, and align natural language to programmatic commands when concrete table schemas are provided. Unlike encoder-based models, we have pushed the boundaries of “HiddenTables” to not be limited by the number of rows - therefore we exhibit improved efficiency in prompt and completion tokens. Our infrastructure has spawned a new dataset “PyQTax” that spans across 116,671 question-table-answer triplets and provides additional fine-grained breakdowns and labels for varying question taxonomies. Therefore, in tandem with our academic contributions regarding LLMs’ deficiency in TableQA tasks, “HiddenTables” is a tactile manifestation of how LLMs can interact with massive datasets while ensuring data security and minimizing generation costs.   \n",
       "1547                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.   \n",
       "1562                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Previous approaches for automatic lay summarisation are exclusively reliant on the source article that, given it is written for a technical audience (e.g., researchers), is unlikely to explicitly define all technical concepts or state all of the background information that is relevant for a lay audience. We address this issue by augmenting eLife, an existing biomedical lay summarisation dataset, with article-specific knowledge graphs, each containing detailed information on relevant biomedical concepts. Using both automatic and human evaluations, we systematically investigate the effectiveness of three different approaches for incorporating knowledge graphs within lay summarisation models, with each method targeting a distinct area of the encoder-decoder model architecture. Our results confirm that integrating graph-based domain knowledge can significantly benefit lay summarisation by substantially increasing the readability of generated text and improving the explanation of technical concepts.   \n",
       "1601                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. One of the primary challenges in this task is bridging the gap between the diverse structures of the KG and the target text, while preserving the details of the input KG. To address this, we propose a novel approach that efficiently integrates graph structure-aware modules with pre-trained language models. Unlike conventional techniques, which only consider direct connections between first-order neighbors, our method delves deeper by incorporating Relative Distance Encoding as a bias within the graph structure-aware module. This enables our model to better capture the intricate topology information present in the KG. To further elevate the fidelity of the generated text, Planning Selection and Similarity Distinction are introduced. Our approach filters the most relevant linearized sequences by employing a planning scorer, while simultaneously distinguishing similar input KGs through contrastive learning techniques. Experiments on two datasets demonstrate the superiority of our model.   \n",
       "1834                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In generating natural language descriptions for knowledge graph triples, prior works used either small-scale, human-annotated datasets or datasets with limited variety of graph shapes, e.g., those having mostly star graphs. Graph-to-text models trained and evaluated on such datasets are largely not assessed for more realistic large-scale, open-domain settings. We introduce a new dataset, GraphNarrative, to fill this gap. Fine-tuning transformer-based pre-trained language models has achieved state-of-the-art performance among graph-to-text models. However, this method suffers from information hallucination—the generated text may contain fabricated facts not present in input graphs. We propose a novel approach that, given a graph-sentence pair in GraphNarrative, trims the sentence to eliminate portions that are not present in the corresponding graph, by utilizing the sentence’s dependency parse tree. Our experiment results verify this approach using models trained on GraphNarrative and existing datasets. The dataset, source code, and trained models are released at https://github.com/idirlab/graphnarrator.   \n",
       "1961                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures — relational, multi-table, and hierarchical matrix shapes — and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.   \n",
       "1978                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibiting remarkable reasoning and comprehension abilities in Natural Language Processing (NLP), most Question Answering (QA) research has primarily centered around general QA tasks based on GPT, neglecting the specific challenges posed by Complex Table QA. In this paper, we propose to incorporate GPT-3.5 to address such challenges, in which complex tables are reconstructed into tuples and specific prompt designs are employed for dialogues. Specifically, we encode each cell’s hierarchical structure, position information, and content as a tuple. By enhancing the prompt template with an explanatory description of the meaning of each tuple and the logical reasoning process of the task, we effectively improve the hierarchical structure awareness capability of GPT-3.5 to better parse the complex tables. Extensive experiments and results on Complex Table QA datasets, i.e., the open-domain dataset HiTAB and the aviation domain dataset AIT-QA show that our approach significantly outperforms previous work on both datasets, leading to state-of-the-art (SOTA) performance.   \n",
       "2046                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Knowledge-grounded dialogue is a task of gener- ating an informative response based on both the dialogue history and external knowledge source. In general, there are two forms of knowledge: manu- ally annotated knowledge graphs and knowledge text from website. From various evaluation viewpoints, each type of knowledge has advantages and downsides. To further distinguish the principles and determinants from the intricate factors, we conduct a thorough experiment and study on the task to answer three essential questions. The ques- tions involve the choice of appropriate knowledge form, the degree of mutual effects between knowl- edge and the model selection, and the few-shot performance of knowledge. Supported by statistical shreds of evidence, we offer conclusive solutions and sensible suggestions for directions and standards of future research.   \n",
       "2067                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Tabular question answering (TQA) presents a challenging setting for neural systems by requiring joint reasoning of natural language with large amounts of semi-structured data. Unlike humans who use programmatic tools like filters to transform data before processing, language models in TQA process tables directly, resulting in information loss as table size increases. In this paper we propose ToolWriter to generate query specific programs and detect when to apply them to transform tables and align them with the TQA model’s capabilities. Focusing Toolwriter to generate row-filtering tools improves the state-of-the-art for WikiTableQuestions and WikiSQL with the most performance gained on long tables. By investigating headroom, our work highlights the broader potential for programmatic tools combined with neural components to manipulate large amounts of structured data.   \n",
       "2150                                                                                                                                                                                                                                                                                                                                                                                                         Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.   \n",
       "2233                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Answering factual questions with temporal intent over knowledge graphs (temporal KGQA) attracts rising attention in recent years.In the generation of temporal queries, existing KGQA methods ignore the fact that some intrinsic connections between events can make them temporally related, which may limit their capability.We systematically analyze the possible interpretation of temporal constraints and conclude the interpretation structures as the Semantic Framework of Temporal Constraints, SF-TCons.Based on the semantic framework, we propose a temporal question answering method, SF-TQA, which generates query graphs by exploring the relevant facts of mentioned entities, where the exploring process is restricted by SF-TCons. Our evaluations show that SF-TQA significantly outperforms existing methods on two benchmarks over different knowledge graphs.   \n",
       "2282                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Pre-trained Language Models (PLMs) have shown effectiveness in various Natural Language Processing (NLP) tasks. Denoising autoencoder is one of the most successful pre-training frameworks, learning to recompose the original text given a noise-corrupted one. The existing studies mainly focus on injecting noises into the input. This paper introduces a simple yet effective pre-training paradigm, equipped with a knowledge-enhanced decoder that predicts the next entity token with noises in the prefix, explicitly strengthening the representation learning of entities that span over multiple input tokens. Specifically, when predicting the next token within an entity, we feed masks into the prefix in place of some of the previous ground-truth tokens that constitute the entity. Our model achieves new state-of-the-art results on two knowledge-driven data-to-text generation tasks with up to 2% BLEU gains.   \n",
       "2385                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Practical dialog systems need to deal with various knowledge sources, noisy user expressions, and the shortage of annotated data. To better solve the above problems, we propose CGoDial, a new challenging and comprehensive Chinese benchmark for multi-domain Goal-oriented Dialog evaluation. It contains 96,763 dialog sessions, and 574,949 dialog turns totally, covering three datasets with different knowledge sources: 1) a slot-based dialog (SBD) dataset with table-formed knowledge, 2) a flow-based dialog (FBD) dataset with tree-formed knowledge, and a retrieval-based dialog (RBD) dataset with candidate-formed knowledge. To bridge the gap between academic benchmarks and spoken dialog scenarios, we either collect data from real conversations or add spoken features to existing datasets via crowd-sourcing. The proposed experimental settings include the combinations of training with either the entire training set or a few-shot training set, and testing with either the standard test set or a hard test subset, which can assess model capabilities in terms of general prediction, fast adaptability and reliable robustness.   \n",
       "2392                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Existing methods on knowledge base question generation (KBQG) learn a one-size-fits-all model by training together all subgraphs without distinguishing the diverse semantics of subgraphs. In this work, we show that making use of the past experience on semantically similar subgraphs can reduce the learning difficulty and promote the performance of KBQG models. To achieve this, we propose a novel approach to model diverse subgraphs with meta-learner (DSM). Specifically, we devise a graph contrastive learning-based retriever to identify semantically similar subgraphs, so that we can construct the semantics-aware learning tasks for the meta-learner to learn semantics-specific and semantics-agnostic knowledge on and across these tasks. Extensive experiments on two widely-adopted benchmarks for KBQG show that DSM derives new state-of-the-art performance and benefits the question answering tasks as a means of data augmentation.   \n",
       "2432                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text (G2T) generation by processing the linearised version of a graph. However, the linearisation is known to ignore the structural information. Additionally, PLMs are typically pre-trained on free text which introduces domain mismatch between pre-training and downstream G2T generation tasks. To address these shortcomings, we propose graph masking pre-training strategies that neither require supervision signals nor adjust the architecture of the underlying pre-trained encoder-decoder model. When used with a pre-trained T5, our approach achieves new state-of-the-art results on WebNLG+2020 and EventNarrative G2T generation datasets. Our method also shows to be very effective in the low-resource setting.   \n",
       "2471                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Natural Language Generation (NLG) for non-English languages is hampered by the scarcity of datasets in these languages. We present the IndicNLG Benchmark, a collection of datasets for benchmarking NLG for 11 Indic languages. We focus on five diverse tasks, namely, biography generation using Wikipedia infoboxes, news headline generation, sentence summarization, paraphrase generation and, question generation. We describe the created datasets and use them to benchmark the performance of several monolingual and multilingual baselines that leverage pre-trained sequence-to-sequence models. Our results exhibit the strong performance of multilingual language-specific pre-trained models, and the utility of models trained on our dataset for other related NLG tasks. Our dataset creation methods can be easily applied to modest-resource languages as they involve simple steps such as scraping news articles and Wikipedia infoboxes, light cleaning, and pivoting through machine translation data. To the best of our knowledge, the IndicNLG Benchmark is the first NLG benchmark for Indic languages and the most diverse multilingual NLG dataset, with approximately 8M examples across 5 tasks and 11 languages. The datasets and models will be publicly available.   \n",
       "2481                                                The aim of Logic2Text is to generate controllable and faithful texts conditioned on tables and logical forms, which not only requires a deep understanding of the tables and logical forms, but also warrants symbolic reasoning over the tables according to the logical forms. State-of-the-art methods based on pre-trained models have achieved remarkable performance on the standard test dataset. However, we question whether these methods really learn how to perform logical reasoning, rather than just relying on the spurious correlations between the headers of the tables and operators of the logical form. To verify this hypothesis, we manually construct a set of counterfactual samples, which modify the original logical forms to generate counterfactual logical forms with rare co-occurred headers and operators and corresponding counterfactual references. SOTA methods give much worse results on these counterfactual samples compared with the results on the original test dataset, which verifies our hypothesis. To deal with this problem, we firstly analyze this bias from a causal perspective, based on which we propose two approaches to reduce the model’s reliance on the shortcut. The first one incorporates the hierarchical structure of the logical forms into the model. The second one exploits automatically generated counterfactual data for training. Automatic and manual experimental results on the original test dataset and counterfactual dataset show that our method is effective to alleviate the spurious correlation. Our work points out the weakness of current methods and takes a further step toward developing Logic2Text models with real logical reasoning ability.   \n",
       "2484                                                                                                                                                                                                                                                                                             Logical table-to-text generation is a task that involves generating logically faithful sentences from tables, which requires models to derive logical-level facts from table records via logical inference. It raises a new challenge on the logical-level content planning of table-to-text models. However, directly learning the logical inference knowledge from table-text pairs is very difficult for neural models because of the ambiguity of natural language and the scarcity of parallel data. Hence even large-scale pre-trained language models present low logical fidelity on logical table-to-text. In this work, we propose a Pretrained Logical Form Generator (PLOG) framework to improve generation fidelity. Specifically, PLOG is first pretrained on a table-to-logical-form generation (table-to-logic) task, then finetuned on downstream table-to-text tasks. The logical forms are formally defined with unambiguous semantics. Hence we can collect a large amount of accurate logical forms from tables without human annotation. In addition, PLOG can learn logical inference from table-logic pairs much more reliably than from table-text pairs. To evaluate our model, we further collect a controlled logical table-to-text dataset CONTLOG based on an existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms strong baselines by a large margin on the logical fidelity, demonstrating the effectiveness of table-to-logic pretraining.   \n",
       "2559                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We propose DEER (Descriptive Knowledge Graph for Explaining Entity Relationships) - an open and informative form of modeling entity relationships. In DEER, relationships between entities are represented by free-text relation descriptions. For instance, the relationship between entities of machine learning and algorithm can be represented as “Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.” To construct DEER, we propose a self-supervised learning method to extract relation descriptions with the analysis of dependency patterns and generate relation descriptions with a transformer-based relation description synthesizing model, where no human labeling is required. Experiments demonstrate that our system can extract and generate high-quality relation descriptions for explaining entity relationships. The results suggest that we can build an open and informative knowledge graph without human annotation.   \n",
       "2573                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Knowledge-enhanced language representation learning has shown promising results across various knowledge-intensive NLP tasks. However, prior methods are limited in efficient utilization of multilingual knowledge graph (KG) data for language model (LM) pretraining. They often train LMs with KGs in indirect ways, relying on extra entity/relation embeddings to facilitate knowledge injection. In this work, we explore methods to make better use of the multilingual annotation and language agnostic property of KG triples, and present novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples. We first generate a large amount of multilingual synthetic sentences using the Wikidata KG triples. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to enable the LMs to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual tasks, including named entity recognition (NER), factual knowledge retrieval, relation classification, and a newly designed logical reasoning task.   \n",
       "2575                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Unfaithful text generation is a common problem for text generation systems. In the case of Data-to-Text (D2T) systems, the factuality of the generated text is particularly crucial for any real-world applications. We introduce R2D2, a training framework that addresses unfaithful Data-to-Text generation by training a system both as a generator and a faithfulness discriminator with additional replacement detection and unlikelihood learning tasks. To facilitate such training, we propose two methods for sampling unfaithful sentences. We argue that the poor entity retrieval capability of D2T systems is one of the primary sources of unfaithfulness, so in addition to the existing metrics, we further propose named entity based metrics to evaluate the fidelity of D2T generations. Our experimental results show that R2D2 systems could effectively mitigate the unfaithful text generation, and they achieve new state-of-theart results on FeTaQA, LogicNLG, and ToTTo, all with significant improvements.   \n",
       "2580                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      To facilitate conversational question answering (CQA) over hybrid contexts in finance, we present a new dataset, named PACIFIC. Compared with existing CQA datasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical reasoning, and (iii) hybrid context of tables and text. A new task is defined accordingly to study Proactive Conversational Question Answering (PCQA), which combines clarification question generation and CQA. In addition, we propose a novel method, namely UniPCQA, to adapt a hybrid format of input and output content in PCQA into the Seq2Seq problem, including the reformulation of the numerical reasoning process as code generation. UniPCQA performs multi-task learning over all sub-tasks in PCQA and incorporates a simple ensemble strategy to alleviate the error propagation issue in the multi-task learning by cross-validating top-k sampled Seq2Seq outputs. We benchmark the PACIFIC dataset with extensive baselines and provide comprehensive evaluations on each sub-task of PCQA.   \n",
       "2619                                                                                                                                                                                                                                                                                                                                                                                                                              Question answering requiring discrete reasoning, e.g., arithmetic computing, comparison, and counting, over knowledge is a challenging task.In this paper, we propose UniRPG, a semantic-parsing-based approach advanced in interpretability and scalability, to perform Unified discrete Reasoning over heterogeneous knowledge resources, i.e., table and text, as Program Generation. Concretely, UniRPG consists of a neural programmer and a symbolic program executor,where a program is the composition of a set of pre-defined general atomic and higher-order operations and arguments extracted from table and text.First, the programmer parses a question into a program by generating operations and copying arguments, and then, the executor derives answers from table and text based on the program.To alleviate the costly program annotation issue, we design a distant supervision approach for programmer learning, where pseudo programs are automatically constructed without annotated derivations.Extensive experiments on the TAT-QA dataset show that UniRPG achieves tremendous improvements and enhances interpretability and scalability compared with previous state-of-the-art methods, even without derivation annotation.Moreover, it achieves promising performance on the textual dataset DROP without derivation annotation.   \n",
       "2673                                                                                                                                                                                                                                                                                                                                                                                                                                            Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables. Large-scale pretrained language models sound like a promising solution to tackle such issues. However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored. Besides, another challenge of integrating the deliberation mechanism into the text-to-text pretrained model for solving the table-to-text task remains seldom studied. In this paper, to implement the table-to-text generation with pretrained language model, we propose a table structure understanding and text deliberating approach, namely TASD. To be specific, we devise a three-layered multi-head attention network to realize the table-structureaware text generation model with the help of the pretrained language model. Furthermore, a multi-pass decoder framework is adopted to enhance the capability of polishing generated text for table descriptions. The empirical studies, as well as human evaluation, on two public datasets, validate that our approach can generate faithful and fluent descriptive texts for different types of tables.   \n",
       "2716                                                                                                                                                                                                                                                                                                                                                                                                                                                      Parsing natural language questions into executable logical forms is a useful and interpretable way to perform question answering on structured data such as knowledge bases (KB) or databases (DB). However, existing approaches on semantic parsing cannot adapt to both modalities, as they suffer from the exponential growth of the logical form candidates and can hardly generalize to unseen data.In this work, we propose Uni-Parser, a unified semantic parser for question answering (QA) on both KB and DB. We define the primitive (relation and entity in KB, and table name, column name and cell value in DB) as the essential element in our framework. The number of primitives grows only at a linear rate to the number of retrieved relations in KB and DB, preventing us from exponential logic form candidates. We leverage the generator to predict final logical forms by altering and composing top-ranked primitives with different operations (e.g. select, where, count). With sufficiently pruned search space by a contrastive primitive ranker, the generator is empowered to capture the composition of primitives enhancing its generalization ability. We achieve competitive results on multiple KB and DB QA benchmarks with more efficiency, especially in the compositional and zero-shot settings.   \n",
       "2726                                                                                                                                                                                                                                                                                                                                                                   Reasoning over tabular data requires both table structure understanding and a broad set of table reasoning skills. Current models with table-specific architectures and pre-training methods perform well on understanding table structures, but they still struggle with tasks that require various table reasoning skills. In this work, we develop ReasTAP to show that high-level table reasoning skills can be injected into models during pre-training without a complex table-specific architecture design. We define 7 table reasoning skills, such as numerical operation, temporal comparison, and conjunction. Each reasoning skill is associated with one example generator, which synthesizes questions over semi-structured tables according to the sampled templates. We model the table pre-training task as a sequence generation task and pre-train ReasTAP to generate precise answers of the synthetic examples. ReasTAP is evaluated on four benchmarks covering three downstream tasks including 1) WikiSQL-Weak and WikiTQ for Table Question Answering, 2) TabFact for Table Fact Verification, and 3) LogicNLG for Faithful Table-to-Text Generation. Experimental results demonstrate that ReasTAP achieves new state-of-the-art results on all of them and delivers a significant improvement under low-resource setting. Our code is publicly available at https://github.com/Yale-LILY/ReasTAP.   \n",
       "2786                                                                                                                                                                                                                                                                                                                                                           Table-to-text generation has been widely studied in the Natural Language Processing community in the recent years. We give a new perspective to this problem by incorporating signals from both tables as well as associated images to generate relevant text. While tables contain a structured list of facts, images are a rich source of unstructured visual information. For example, in the tourism domain, images can be used to infer knowledge such as the type of landmark (e.g., church), its architecture (e.g., Ancient Roman), and composition (e.g., white marble). Therefore, in this paper, we introduce the novel task of Vision-augmented Table-To-Text Generation (VisToT, defined as follows: given a table and an associated image, produce a descriptive sentence conditioned on the multimodal input. For the task, we present a novel multimodal table-to-text dataset, WikiLandmarks, covering 73,084 unique world landmarks. Further, we also present a competitive architecture, namely, VT3 that generates accurate sentences conditioned on the image and table pairs. Through extensive analyses and experiments, we show that visual cues from images are helpful in (i) inferring missing information from incomplete or sparse tables, and (ii) strengthening the importance of useful information from noisy tables for natural language generation. We make the code and data publicly available.   \n",
       "2951                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Knowledge-grounded dialogue systems are challenging to build due to the lack of training data and heterogeneous knowledge sources. Existing systems perform poorly on unseen topics due to limited topics covered in the training data. In addition, it is challenging to generalize to the domains that require different types of knowledge sources. To address the above challenges, we present PLUG, a language model that homogenizes different knowledge sources to a unified knowledge representation for knowledge-grounded dialogue generation tasks. We first retrieve relevant information from heterogeneous knowledge sources (e.g., wiki, dictionary, or knowledge graph); Then the retrieved knowledge is transformed into text and concatenated with dialogue history to feed into the language model for generating responses. PLUG is pre-trained on a large-scale knowledge-grounded dialogue corpus. The empirical evaluation on two benchmarks shows that PLUG generalizes well across different knowledge-grounded dialogue tasks. It achieves comparable performance with state-of-the-art methods in the fully-supervised setting and significantly outperforms other approaches in zero-shot and few-shot settings.   \n",
       "2993                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A* search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.   \n",
       "3004                                                                                                                                                                                                                                                                                                                                                       The information in tables can be an important complement to text, making table-based question answering (QA) systems of great value. The intrinsic complexity of handling tables often adds an extra burden to both model design and data annotation. In this paper, we aim to develop a simple table-based QA model with minimal annotation effort. Motivated by the fact that table-based QA requires both alignment between questions and tables and the ability to perform complicated reasoning over multiple table elements, we propose an omnivorous pretraining approach that consumes both natural and synthetic data to endow models with these respective abilities. Specifically, given freely available tables, we leverage retrieval to pair them with relevant natural sentences for mask-based pretraining, and synthesize NL questions by converting SQL sampled from tables for pretraining with a QA loss. We perform extensive experiments in both few-shot and full settings, and the results clearly demonstrate the superiority of our model OmniTab, with the best multitasking approach achieving an absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively, also establishing a new state-of-the-art on WikiTableQuestions. Detailed ablations and analyses reveal different characteristics of natural and synthetic data, shedding light on future directions in omnivorous pretraining.   \n",
       "3019                                                                                                                                                                                                                                                                                                                                                                                                         Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have leveraged structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to improve the scale at which knowledge can be obtained. However, training on symbolic KG entities limits their applicability in tasks involving natural language text where they ignore overall context. To mitigate this, we propose a CommonSense Contextualizer (CoSe-Co) conditioned on sentences as input to make it generically usable in tasks for generating knowledge relevant to the overall context of input text. To train CoSe-Co, we propose a novel dataset comprising of sentence and commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and contain novel entities not present in the underlying KG. We augment generated knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets. We also demonstrate its applicability in improving performance of a baseline model for paraphrase generation task.   \n",
       "3049                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size KG, WikiMovies, we saw 3x improvement of exact match score on MetaQA task. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.   \n",
       "3307                                                                                                                                                                                                                                                                                                           Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly, we propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column, and it differentiates between relevant cells and irrelevant cells from the structural perspective. Our framework also modifies the positional encoding mechanism to preserve the relative position of tokens in the same cell but enforce position invariance among different cells. Our technology is free to be plugged into existing table-to-text generation models, and has improved T5-based models to offer better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo, we preserve promising performance, while previous SOTA systems, even with transformation-based data augmentation, have seen significant performance drops.   \n",
       "\n",
       "      pub_year  \\\n",
       "211       2024   \n",
       "248       2024   \n",
       "290       2024   \n",
       "359       2024   \n",
       "542       2024   \n",
       "557       2024   \n",
       "630       2024   \n",
       "697       2024   \n",
       "711       2024   \n",
       "751       2024   \n",
       "824       2023   \n",
       "835       2023   \n",
       "960       2023   \n",
       "968       2023   \n",
       "990       2023   \n",
       "1030      2023   \n",
       "1101      2023   \n",
       "1138      2023   \n",
       "1196      2023   \n",
       "1213      2023   \n",
       "1236      2023   \n",
       "1292      2023   \n",
       "1327      2023   \n",
       "1414      2023   \n",
       "1453      2023   \n",
       "1506      2023   \n",
       "1547      2023   \n",
       "1562      2023   \n",
       "1601      2023   \n",
       "1834      2023   \n",
       "1961      2023   \n",
       "1978      2023   \n",
       "2046      2023   \n",
       "2067      2023   \n",
       "2150      2022   \n",
       "2233      2022   \n",
       "2282      2022   \n",
       "2385      2022   \n",
       "2392      2022   \n",
       "2432      2022   \n",
       "2471      2022   \n",
       "2481      2022   \n",
       "2484      2022   \n",
       "2559      2022   \n",
       "2573      2022   \n",
       "2575      2022   \n",
       "2580      2022   \n",
       "2619      2022   \n",
       "2673      2022   \n",
       "2716      2022   \n",
       "2726      2022   \n",
       "2786      2022   \n",
       "2951      2022   \n",
       "2993      2022   \n",
       "3004      2022   \n",
       "3019      2022   \n",
       "3049      2022   \n",
       "3307      2022   \n",
       "\n",
       "                                                                   relevancy_status  \\\n",
       "211     ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "248               ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "290               ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "359               ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "542               ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "557             ```json\\n{'status': true, 'class': 'Table-to-text generation'}\\n```   \n",
       "630               ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "697     ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "711               ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "751            ```json\\n{'status':true, 'class':'database-to-text generation'}\\n```   \n",
       "824               ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "835     ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "960     ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "968     ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "990               ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "1030    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "1101    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "1138              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "1196              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "1213              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "1236    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "1292    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "1327    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "1414    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "1453            ```json\\n{'status': true, 'class': 'Table-to-text generation'}\\n```   \n",
       "1506            ```json\\n{'status': true, 'class': 'Table-to-text generation'}\\n```   \n",
       "1547              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "1562    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "1601  ```json\\n{'status': true, 'class': 'Knowledge graph-to-text generation'}\\n```   \n",
       "1834  ```json\\n{'status': true, 'class': 'Knowledge graph-to-text generation'}\\n```   \n",
       "1961              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "1978              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2046    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "2067              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2150    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "2233    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "2282           ```json\\n{'status':true, 'class':'database-to-text generation'}\\n```   \n",
       "2385              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2392    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "2432    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "2471              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2481              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2484              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2559    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "2573    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "2575              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2580              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2619              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2673              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2716           ```json\\n{'status':true, 'class':'database-to-text generation'}\\n```   \n",
       "2726              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "2786            ```json\\n{'status': true, 'class': 'Table-to-text generation'}\\n```   \n",
       "2951    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "2993              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "3004              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "3019    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "3049    ```json\\n{'status':true, 'class':'Knowledge graph-to-text generation'}\\n```   \n",
       "3307              ```json\\n{'status':true, 'class':'Table-to-text generation'}\\n```   \n",
       "\n",
       "     publication  processed  relevancy                     relevant_domain  \n",
       "211         EACL       True       True  Knowledge graph-to-text generation  \n",
       "248        NAACL       True       True            Table-to-text generation  \n",
       "290        NAACL       True       True            Table-to-text generation  \n",
       "359        NAACL       True       True            Table-to-text generation  \n",
       "542        NAACL       True       True            Table-to-text generation  \n",
       "557        NAACL       True       True            Table-to-text generation  \n",
       "630        NAACL       True       True            Table-to-text generation  \n",
       "697        NAACL       True       True  Knowledge graph-to-text generation  \n",
       "711        NAACL       True       True            Table-to-text generation  \n",
       "751        NAACL       True       True         database-to-text generation  \n",
       "824         EACL       True       True            Table-to-text generation  \n",
       "835         EACL       True       True  Knowledge graph-to-text generation  \n",
       "960         EACL       True       True  Knowledge graph-to-text generation  \n",
       "968         EACL       True       True  Knowledge graph-to-text generation  \n",
       "990         EACL       True       True            Table-to-text generation  \n",
       "1030        EACL       True       True  Knowledge graph-to-text generation  \n",
       "1101       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "1138       EMNLP       True       True            Table-to-text generation  \n",
       "1196       EMNLP       True       True            Table-to-text generation  \n",
       "1213       EMNLP       True       True            Table-to-text generation  \n",
       "1236       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "1292       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "1327       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "1414       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "1453       EMNLP       True       True            Table-to-text generation  \n",
       "1506       EMNLP       True       True            Table-to-text generation  \n",
       "1547       EMNLP       True       True            Table-to-text generation  \n",
       "1562       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "1601       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "1834       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "1961       EMNLP       True       True            Table-to-text generation  \n",
       "1978       EMNLP       True       True            Table-to-text generation  \n",
       "2046       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "2067       EMNLP       True       True            Table-to-text generation  \n",
       "2150       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "2233       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "2282       EMNLP       True       True         database-to-text generation  \n",
       "2385       EMNLP       True       True            Table-to-text generation  \n",
       "2392       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "2432       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "2471       EMNLP       True       True            Table-to-text generation  \n",
       "2481       EMNLP       True       True            Table-to-text generation  \n",
       "2484       EMNLP       True       True            Table-to-text generation  \n",
       "2559       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "2573       EMNLP       True       True  Knowledge graph-to-text generation  \n",
       "2575       EMNLP       True       True            Table-to-text generation  \n",
       "2580       EMNLP       True       True            Table-to-text generation  \n",
       "2619       EMNLP       True       True            Table-to-text generation  \n",
       "2673       EMNLP       True       True            Table-to-text generation  \n",
       "2716       EMNLP       True       True         database-to-text generation  \n",
       "2726       EMNLP       True       True            Table-to-text generation  \n",
       "2786       EMNLP       True       True            Table-to-text generation  \n",
       "2951       NAACL       True       True  Knowledge graph-to-text generation  \n",
       "2993       NAACL       True       True            Table-to-text generation  \n",
       "3004       NAACL       True       True            Table-to-text generation  \n",
       "3019       NAACL       True       True  Knowledge graph-to-text generation  \n",
       "3049       NAACL       True       True  Knowledge graph-to-text generation  \n",
       "3307       NAACL       True       True            Table-to-text generation  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_json_values(\"```json\\n{'status': true, 'class': 'Knowledge graph-to-text generation'}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
