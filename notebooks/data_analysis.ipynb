{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from acl_papers.analysis import analyse_abstract\n",
    "from acl_papers.modules import check_relevant\n",
    "from acl_papers import ACL_PAPER_PATH\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) \n",
    "\n",
    "MODEL=\"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.listdir(ACL_PAPER_PATH)\n",
    "data_path = os.path.join(ACL_PAPER_PATH, path[0])\n",
    "\n",
    "df = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row['processed']:\n",
    "        continue\n",
    "    else:\n",
    "        verdict=analyse_abstract(title=row['title'], abstract=row['abstract'], model=MODEL)\n",
    "        df.at[i, 'relevancy_status'] = verdict\n",
    "        df.at[i, 'processed'] = True\n",
    "        df.to_csv(data_path)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>paper_type</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>relevancy_status</th>\n",
       "      <th>publication</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>3369</td>\n",
       "      <td>3369</td>\n",
       "      <td>3369</td>\n",
       "      <td>3369</td>\n",
       "      <td>main</td>\n",
       "      <td>Hate Speech and Counter Speech Detection: Conversational Context Does Matter</td>\n",
       "      <td>Hate speech is plaguing the cyberspace along with user-generated content. Adding counter speech has become an effective way to combat hate speech online. Existing datasets and models target either (a) hate speech or (b) hate and counter speech but disregard the context. This paper investigates the role of context in the annotation and detection of online hate and counter speech, where context is defined as the preceding comment in a conversation thread. We created a context-aware dataset for a 3-way classification task on Reddit comments: hate speech, counter speech, or neutral. Our analyses indicate that context is critical to identify hate and counter speech: human judgments change for most comments depending on whether we show annotators the context. A linguistic analysis draws insights into the language people use to express hate and counter speech. Experimental results show that neural networks obtain significantly better results if context is taken into account. We also present qualitative error analyses shedding light into (a) when and why context is beneficial and (b) the remaining errors made by our best model when context is taken into account.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>3370</td>\n",
       "      <td>3370</td>\n",
       "      <td>3370</td>\n",
       "      <td>3370</td>\n",
       "      <td>main</td>\n",
       "      <td>DACSA: A large-scale Dataset for Automatic summarization of Catalan and Spanish newspaper Articles</td>\n",
       "      <td>The application of supervised methods to automatic summarization requires the availability of adequate corpora consisting of a set of document-summary pairs. As in most Natural Language Processing tasks, the great majority of available datasets for summarization are in English, making it difficult to develop automatic summarization models for other languages. Although Spanish is gradually forming part of some recent summarization corpora, it is not the same for minority languages such as Catalan. In this work, we describe the construction of a corpus of Catalan and Spanish newspapers, the Dataset for Automatic summarization of Catalan and Spanish newspaper Articles (DACSA) corpus. It is a high-quality large-scale corpus that can be used to train summarization models for Catalan and Spanish.We have carried out an analysis of the corpus, both in terms of the style of the summaries and the difficulty of the summarization task. In particular, we have used a set of well-known metrics in the summarization field in order to characterize the corpus. Additionally, for benchmarking purposes, we have evaluated the performances of some extractive and abstractive summarization systems on the DACSA corpus.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3371</th>\n",
       "      <td>3371</td>\n",
       "      <td>3371</td>\n",
       "      <td>3371</td>\n",
       "      <td>3371</td>\n",
       "      <td>main</td>\n",
       "      <td>Time Waits for No One! Analysis and Challenges of Temporal Misalignment</td>\n",
       "      <td>When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain-specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal misalignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously reported. We also find that, while temporal adaptation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>3372</td>\n",
       "      <td>3372</td>\n",
       "      <td>3372</td>\n",
       "      <td>3372</td>\n",
       "      <td>main</td>\n",
       "      <td>MCSE: Multimodal Contrastive Learning of Sentence Embeddings</td>\n",
       "      <td>Learning semantically meaningful sentence embeddings is an open problem in natural language processing. In this work, we propose a sentence embedding learning approach that exploits both visual and textual information via a multimodal contrastive objective. Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. In particular, combining a small amount of multimodal data with a large text-only corpus, we improve the state-of-the-art average Spearmanâ€™s correlation by 1.7%. By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>3373</td>\n",
       "      <td>3373</td>\n",
       "      <td>3373</td>\n",
       "      <td>3373</td>\n",
       "      <td>main</td>\n",
       "      <td>HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction</td>\n",
       "      <td>Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar. To overcome these defects, we propose a novel contrastive learning framework named HiURE, which has the capability to derive hierarchical signals from relational feature space using cross hierarchy attention and effectively optimize relation representation of sentences under exemplar-wise contrastive learning. Experimental results on two public datasets demonstrate the advanced effectiveness and robustness of HiURE on unsupervised relation extraction when compared with state-of-the-art models.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>3374</td>\n",
       "      <td>3374</td>\n",
       "      <td>3374</td>\n",
       "      <td>3374</td>\n",
       "      <td>main</td>\n",
       "      <td>Diagnosing Vision-and-Language Navigation: What Really Matters</td>\n",
       "      <td>Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, there still exist non-negligible gaps between machinesâ€™ performance and human benchmarks. Moreover, the agentsâ€™ inner mechanisms for navigation decisions remain unclear. To the best of our knowledge, how the agents perceive the multimodal input is under-studied and needs investigation. In this work, we conduct a series of diagnostic experiments to unveil agentsâ€™ focus during navigation. Results show that indoor navigation agents refer to both object and direction tokens when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and poorly understand the object tokens. Transformer-based agents acquire a better cross-modal understanding of objects and display strong numerical reasoning ability than non-Transformer-based agents. When it comes to vision-and-language alignments, many models claim that they can align object tokens with specific visual targets. We find unbalanced attention on the vision and text input and doubt the reliability of such cross-modal alignments.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>3375</td>\n",
       "      <td>3375</td>\n",
       "      <td>3375</td>\n",
       "      <td>3375</td>\n",
       "      <td>main</td>\n",
       "      <td>Aligning to Social Norms and Values in Interactive Narratives</td>\n",
       "      <td>We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based gamesâ€”environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal normsâ€”causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of peopleâ€”in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others. We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>3376</td>\n",
       "      <td>3376</td>\n",
       "      <td>3376</td>\n",
       "      <td>3376</td>\n",
       "      <td>main</td>\n",
       "      <td>MOVER: Mask, Over-generate and Rank for Hyperbole Generation</td>\n",
       "      <td>Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale English hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole generation that does not require parallel literal-hyperbole pairs. During training, we fine-tune BART to infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of an input literal sentence and over-generate multiple possible hyperbolic versions. Then a BERT-based ranker selects the best candidate by hyperbolicity and paraphrase quality. Automatic and human evaluation results show that our model is effective at generating hyperbolic paraphrase sentences and outperforms several baseline systems.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>3377</td>\n",
       "      <td>3377</td>\n",
       "      <td>3377</td>\n",
       "      <td>3377</td>\n",
       "      <td>main</td>\n",
       "      <td>Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference</td>\n",
       "      <td>The task of natural language inference (NLI), to decide if a hypothesis entails or contradicts a premise, received considerable attention in recent years. All competitive systems build on top of contextualized representations and make use of transformer architectures for learning an NLI model. When somebody is faced with a particular NLI task, they need to select the best model that is available. This is a time-consuming and resource-intense endeavour. To solve this practical problem, we propose a simple method for predicting the performance without actually fine-tuning the model. We do this by testing how well the pre-trained models perform on the aNLI task when just comparing sentence embeddings with cosine similarity to what kind of performance is achieved when training a classifier on top of these embeddings. We show that the accuracy of the cosine similarity approach correlates strongly with the accuracy of the classification approach with a Pearson correlation coefficient of 0.65. Since the similarity is orders of magnitude faster to compute on a given dataset (less than a minute vs. hours), our method can lead to significant time savings in the process of model selection.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>3378</td>\n",
       "      <td>3378</td>\n",
       "      <td>3378</td>\n",
       "      <td>3378</td>\n",
       "      <td>main</td>\n",
       "      <td>Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics</td>\n",
       "      <td>How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice. This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small.</td>\n",
       "      <td>2022</td>\n",
       "      <td>```json\\n{'status':false, 'class':null}\\n```</td>\n",
       "      <td>NAACL</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0 paper_type  \\\n",
       "3369          3369          3369          3369        3369       main   \n",
       "3370          3370          3370          3370        3370       main   \n",
       "3371          3371          3371          3371        3371       main   \n",
       "3372          3372          3372          3372        3372       main   \n",
       "3373          3373          3373          3373        3373       main   \n",
       "3374          3374          3374          3374        3374       main   \n",
       "3375          3375          3375          3375        3375       main   \n",
       "3376          3376          3376          3376        3376       main   \n",
       "3377          3377          3377          3377        3377       main   \n",
       "3378          3378          3378          3378        3378       main   \n",
       "\n",
       "                                                                                                   title  \\\n",
       "3369                        Hate Speech and Counter Speech Detection: Conversational Context Does Matter   \n",
       "3370  DACSA: A large-scale Dataset for Automatic summarization of Catalan and Spanish newspaper Articles   \n",
       "3371                             Time Waits for No One! Analysis and Challenges of Temporal Misalignment   \n",
       "3372                                        MCSE: Multimodal Contrastive Learning of Sentence Embeddings   \n",
       "3373              HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction   \n",
       "3374                                      Diagnosing Vision-and-Language Navigation: What Really Matters   \n",
       "3375                                       Aligning to Social Norms and Values in Interactive Narratives   \n",
       "3376                                        MOVER: Mask, Over-generate and Rank for Hyperbole Generation   \n",
       "3377               Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference   \n",
       "3378                Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             abstract  \\\n",
       "3369                                                                                                                                                                                                                                                                                                                                                             Hate speech is plaguing the cyberspace along with user-generated content. Adding counter speech has become an effective way to combat hate speech online. Existing datasets and models target either (a) hate speech or (b) hate and counter speech but disregard the context. This paper investigates the role of context in the annotation and detection of online hate and counter speech, where context is defined as the preceding comment in a conversation thread. We created a context-aware dataset for a 3-way classification task on Reddit comments: hate speech, counter speech, or neutral. Our analyses indicate that context is critical to identify hate and counter speech: human judgments change for most comments depending on whether we show annotators the context. A linguistic analysis draws insights into the language people use to express hate and counter speech. Experimental results show that neural networks obtain significantly better results if context is taken into account. We also present qualitative error analyses shedding light into (a) when and why context is beneficial and (b) the remaining errors made by our best model when context is taken into account.   \n",
       "3370                                                                                                                                                                                                                                                                                                                      The application of supervised methods to automatic summarization requires the availability of adequate corpora consisting of a set of document-summary pairs. As in most Natural Language Processing tasks, the great majority of available datasets for summarization are in English, making it difficult to develop automatic summarization models for other languages. Although Spanish is gradually forming part of some recent summarization corpora, it is not the same for minority languages such as Catalan. In this work, we describe the construction of a corpus of Catalan and Spanish newspapers, the Dataset for Automatic summarization of Catalan and Spanish newspaper Articles (DACSA) corpus. It is a high-quality large-scale corpus that can be used to train summarization models for Catalan and Spanish.We have carried out an analysis of the corpus, both in terms of the style of the summaries and the difficulty of the summarization task. In particular, we have used a set of well-known metrics in the summarization field in order to characterize the corpus. Additionally, for benchmarking purposes, we have evaluated the performances of some extractive and abstractive summarization systems on the DACSA corpus.   \n",
       "3371                                                                                                                                                                                                                                                                                                                                                                                                                                                                  When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain-specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal misalignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously reported. We also find that, while temporal adaptation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models.   \n",
       "3372                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Learning semantically meaningful sentence embeddings is an open problem in natural language processing. In this work, we propose a sentence embedding learning approach that exploits both visual and textual information via a multimodal contrastive objective. Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. In particular, combining a small amount of multimodal data with a large text-only corpus, we improve the state-of-the-art average Spearmanâ€™s correlation by 1.7%. By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.   \n",
       "3373                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar. To overcome these defects, we propose a novel contrastive learning framework named HiURE, which has the capability to derive hierarchical signals from relational feature space using cross hierarchy attention and effectively optimize relation representation of sentences under exemplar-wise contrastive learning. Experimental results on two public datasets demonstrate the advanced effectiveness and robustness of HiURE on unsupervised relation extraction when compared with state-of-the-art models.   \n",
       "3374                                                                                                                                                                                                               Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, there still exist non-negligible gaps between machinesâ€™ performance and human benchmarks. Moreover, the agentsâ€™ inner mechanisms for navigation decisions remain unclear. To the best of our knowledge, how the agents perceive the multimodal input is under-studied and needs investigation. In this work, we conduct a series of diagnostic experiments to unveil agentsâ€™ focus during navigation. Results show that indoor navigation agents refer to both object and direction tokens when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and poorly understand the object tokens. Transformer-based agents acquire a better cross-modal understanding of objects and display strong numerical reasoning ability than non-Transformer-based agents. When it comes to vision-and-language alignments, many models claim that they can align object tokens with specific visual targets. We find unbalanced attention on the vision and text input and doubt the reliability of such cross-modal alignments.   \n",
       "3375  We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based gamesâ€”environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal normsâ€”causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of peopleâ€”in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others. We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.   \n",
       "3376                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale English hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole generation that does not require parallel literal-hyperbole pairs. During training, we fine-tune BART to infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of an input literal sentence and over-generate multiple possible hyperbolic versions. Then a BERT-based ranker selects the best candidate by hyperbolicity and paraphrase quality. Automatic and human evaluation results show that our model is effective at generating hyperbolic paraphrase sentences and outperforms several baseline systems.   \n",
       "3377                                                                                                                                                                                                                                                                                                                                    The task of natural language inference (NLI), to decide if a hypothesis entails or contradicts a premise, received considerable attention in recent years. All competitive systems build on top of contextualized representations and make use of transformer architectures for learning an NLI model. When somebody is faced with a particular NLI task, they need to select the best model that is available. This is a time-consuming and resource-intense endeavour. To solve this practical problem, we propose a simple method for predicting the performance without actually fine-tuning the model. We do this by testing how well the pre-trained models perform on the aNLI task when just comparing sentence embeddings with cosine similarity to what kind of performance is achieved when training a classifier on top of these embeddings. We show that the accuracy of the cosine similarity approach correlates strongly with the accuracy of the classification approach with a Pearson correlation coefficient of 0.65. Since the similarity is orders of magnitude faster to compute on a given dataset (less than a minute vs. hours), our method can lead to significant time savings in the process of model selection.   \n",
       "3378                                                                                                                                                                                                                                                                                                                                                                                                                                        How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice. This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small.   \n",
       "\n",
       "      pub_year                              relevancy_status publication  \\\n",
       "3369      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3370      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3371      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3372      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3373      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3374      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3375      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3376      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3377      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "3378      2022  ```json\\n{'status':false, 'class':null}\\n```       NAACL   \n",
       "\n",
       "      processed  \n",
       "3369       True  \n",
       "3370       True  \n",
       "3371       True  \n",
       "3372       True  \n",
       "3373       True  \n",
       "3374       True  \n",
       "3375       True  \n",
       "3376       True  \n",
       "3377       True  \n",
       "3378       True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
